{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x00000288421F74A0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x00000288422378F0>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir('data/')\n",
    "pdf_documents = []\n",
    "for i in dir_list:\n",
    "    loader_pdf = PyPDFLoader('data/' + i)\n",
    "    pdf_documents.extend(loader_pdf.load())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/1.pdf', 'page': 0}, page_content='International Journal of Computer Applications (0975 – 8887)  \\nVolume 161 – No 11, March 2017  \\n1 Credit Scoring using Machine Learning Techniques  \\nSunil  Bhatia  \\nComputer Science Department  \\nVESIT, Chembur  \\nMumbai University  \\n Pratik  Sharma  \\nComputer Science Department  \\nVESIT, Chembur  \\nMumbai University  \\n Rohit  Burman  \\nComputer Science Department  \\nVESIT, Chembur  \\nMumbai University  \\n \\nSantosh  Hazari  \\nComputer Science Department  \\nVESIT, Chembur  \\nMumbai  University  Rupali  Hande  \\nComputer Science Department  \\nVESIT, Chembur  \\nMumbai University  \\n \\n \\nABSTRACT  \\nLenders such as banks and credit card companies while \\nreviewing a client‟s request for loan use credit scores . Credit \\nscores help measure the creditworthiness of the client using a \\nnumerical score.  Now it has been found out that the problem \\ncan be optimized by using various st atistical models. In this \\nstudy a wide range of statistical methods in machine learning'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 0}, page_content='numerical score.  Now it has been found out that the problem \\ncan be optimized by using various st atistical models. In this \\nstudy a wide range of statistical methods in machine learning \\nhave been applied, though the datasets available to the public \\nis limited due to confidentiality concerns. Problems particular \\nto the context of credit scoring are exam ined and the statistical \\nmethods are reviewed.  \\nKeywords  \\nData Mining, Credit Scoring, Logistic Regression, LDA, \\nXGBoost, Random Forest.  \\n1. INTRODUCTION  \\nThe process of deciding to accept or rejec t a client‟s credit by \\nbanks is commonly executed via judgmental techniques \\nand/or credit scoring models. Earlier, financial institutions and \\nmost  banks used the method of judgmental approach that  is \\nbased on the 5 C‟s, which  are condition, character, collateral, \\ncapital and capacity. In  this modern computerized  world, this \\nprocess of deciding can be optimized  using statistical methods \\nin machine learning.'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 0}, page_content='capital and capacity. In  this modern computerized  world, this \\nprocess of deciding can be optimized  using statistical methods \\nin machine learning.  \\nThus banks and financial institutions to improve the process \\nof assessing creditworthiness of an applicant during the credit \\nevaluatio n process  develop Credit scoring models . Credit \\nscoring is a system creditors (banks, insurance companies) use \\nto assign credit applicants to either a „„good credit‟‟ group the \\none that is more likely to repay the debt or a „„bad credit‟‟ \\ngroup the one who  has a high possibility of defaulting on debt \\nor any financial obligation i.e. not paying within the given \\ndeadline.  \\nConstruction of credit scoring models requires data mining \\ntechniques. Using,  demographic characteristics, historical data \\non payments and statistical techniques, these models can help \\nin identifying the important demographic characteristics, \\nwhich is related to credit risk,  and assign a score to each'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 0}, page_content='on payments and statistical techniques, these models can help \\nin identifying the important demographic characteristics, \\nwhich is related to credit risk,  and assign a score to each \\ncustomer. The  probability that an applicant will default must \\nbe calculated from information  about the applicant provided at \\nthe time of filing the application, and this estimate will thus \\nserve as the basis for his/her creditworthiness.  \\nIn the paper [ 1] the four machine learning methods reviewed \\nfor Credit scoring jare statistical methods, Hybri d Methods, \\nArtificial Intelligence method, and ensemble learning method. \\nStatistical model includes LDA (Linear Discriminant \\nAnalysis), MARS, Decision tree. AI methods include ANN, SVM, and K -Nearest method. Paper also discusses about \\nbehavioral scoring me thod. Behavioral scoring makes a \\ndecision about management of credit based on the repayment \\nperformance of existing customers during a certain predefined'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 0}, page_content='behavioral scoring me thod. Behavioral scoring makes a \\ndecision about management of credit based on the repayment \\nperformance of existing customers during a certain predefined \\nperiod of time. It also includes repayment behavior and \\npayment history of the client. According to th is paper \\nensemble  learning has better prediction accuracy and \\nclassification ability and is thus widely applied to personal \\ncredit evolution.  \\nThis paper [ 2] deals with the design aspects related to \\nfinancial fraud detection. The aim of feature selection is  to \\nimprove both the actual and computational performance of the \\nsolution, as well as providing a better understanding of the \\nproblem. Feature ranking algorithms assign rating to \\nindividual features based on certain attributes such as \\naccuracy, content and  consistency and choose a suitable \\nsubset on the basis of ranking. Performance metrics are used \\nas small increase in performance can lead to large economic'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 0}, page_content='accuracy, content and  consistency and choose a suitable \\nsubset on the basis of ranking. Performance metrics are used \\nas small increase in performance can lead to large economic \\nbenefits. In Classification method accuracy, sensitivity, \\nspecificity, precision, false positive rat e are the performance \\nmeasure. In clustering Hopkins statistic is the performance \\nmeasure. The paper tests various algorithms such as GA1, \\nGA2, DT1, DT2, SVM etc. for determining the best \\nprediction method. It has been found that if misclassification \\ncosts  are high, techniques with higher sensitivity such as GP1 \\nor neural networks may be suitable choices. If receptiveness \\nto minor chances in dataset is desired then the ant colony \\noptimization or neural networks could be appropriate. Overall \\nthe support vect or machine could be considered to have the \\nbest performance with the highest accuracy.  \\nThe study applies the credit scoring techniques using data'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 0}, page_content='the support vect or machine could be considered to have the \\nbest performance with the highest accuracy.  \\nThe study applies the credit scoring techniques using data \\nmining of payment history of members from a recreational \\nclub [ 3]. Classification performance of credit scorec ard \\nmodel, logistic regression model and decision tree model were \\ncompared. Classification error rates were 27.9%, 28.8% and \\n28.1% respectively . The cut off score also known as the \\nthreshold can be determined by the value of K -S Test for each \\nbucket of sco re in the validation sample. The target variable is \\npayment status which is a binary variable with 2 categories: \\ndefault (consisting of persons who have defaulted) and non -\\ndefault (consisting of person who have not defaulted) which \\nwere coded using numeric al values (1 and 0). Out of 2765 \\nmembers, 35% were found to be defaulters. The majority of \\nthe members are male (80%) and more than half (74%) of the'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 0}, page_content='were coded using numeric al values (1 and 0). Out of 2765 \\nmembers, 35% were found to be defaulters. The majority of \\nthe members are male (80%) and more than half (74%) of the \\ncustomers are from non -government sector. Two main \\nlimitations are the availability of data and sample sel ection \\nissues.'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 1}, page_content='International Journal of Computer Applications (0975 – 8887)  \\nVolume 161 – No 11, March 2017  \\n2 SAS advanced analytical techniques have a proven ability to \\nquickly and accurately forecast the risk of credit losses [ 4]. \\nThis paper uses Data mining in SAS for credit scoring \\nprocess. Application data, financial data, credit bureau data, \\ncharacter data, performance data, demographic data are \\ndifferent types of data used as input to an application \\nscorecard. As data mining is used, Credit scoring is more of \\ninductive classification where the learning process is \\nsupervised by a vector of know n outcomes in the training \\ndata. Decision trees is a technique used where each branch is a \\nclassification question and the leaves of the trees are segments \\nof the data that fall within a specific class. E.g. Good or bad. \\nSplits are made on the basis of a g iven variable and its \\nspecific value. Decision trees can handle missing values'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 1}, page_content='of the data that fall within a specific class. E.g. Good or bad. \\nSplits are made on the basis of a g iven variable and its \\nspecific value. Decision trees can handle missing values \\nwithout imputation and also can divide the data on each \\nbranch without losing any data. It is found that Decision trees \\nand logistic regression are excellent techniques for cred it \\nscoring models. Also decision trees has an advantage that it is \\neasy to understand the result as each leaf node can be traced \\nback to the root node.  \\nCredit scoring is used to describe formal statistical methods \\nfor classifying applicants for credit into  good and bad risk \\nclasses [ 5]. Predictor variables are used to yield estimates of \\nthe probabilities of defaulting. Factors, which need to be \\nconsidered,  are the cost of collecting and analyzing  \\ninformation, expected returns on good and bad loans that have  \\nbeen accepted, the fact that loans may be profitable even if the'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 1}, page_content=\"considered,  are the cost of collecting and analyzing  \\ninformation, expected returns on good and bad loans that have  \\nbeen accepted, the fact that loans may be profitable even if the \\nborrower defaults, the attrition rate etc. Population drift can \\nalso be a problem in credit scoring applications. A widespread \\npractice used in credit scoring is reject inference. It describ es \\nthe practice of attempting to infer the likely true class of the \\nrejected applicants and then using this information to yield a \\nnew scorecard that is superior to one built on only those \\naccepted for credit thus making the dataset more reliable. Risk \\nbased pricing, loan servicing, and review function and fraud \\nscorecard are other issues which are addressed in this paper.  \\n2. Machine Learning Techniques  \\n2.1 Linear Discriminant Analysis  \\nLinear discriminant analysis (LDA) is derived from Fisher's \\nlinear discriminant , a classification method used in machine\"),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 1}, page_content=\"2. Machine Learning Techniques  \\n2.1 Linear Discriminant Analysis  \\nLinear discriminant analysis (LDA) is derived from Fisher's \\nlinear discriminant , a classification method used in machine \\nlearning  to disco ver a linear combination of features that \\ncharacterizes two or more classes of objects. The resulting \\ncombination of the most desired features is used as a \\ndimensionality reduction before later cla ssification or as a \\nlinear classifier.  \\nIf one wants to preserve the difference between the classes as \\nwell while reducing the dimensions, LDA is the option \\navailable that  determines the discriminant dimension in the \\nresponse pattern space, on which the rat io of between -class \\nover within -class variance of the available data is maximized.  \\nLinear Discriminant Analysis method is thus easy and predicts \\nmodels whose accuracy is as good as other methods that  are \\ncomplex.  \\nIn Credit Scoring model,  we determine worth iness to receive\"),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 1}, page_content=\"Linear Discriminant Analysis method is thus easy and predicts \\nmodels whose accuracy is as good as other methods that  are \\ncomplex.  \\nIn Credit Scoring model,  we determine worth iness to receive \\ncredit. This is done by determining the probability that the \\nperson will default in future or not. Thus,  we can use LDA to \\npredict that according to the person's details he/she falls under \\ndefault or not -default category and accordingly grant credit.  \\nThe score function  is calculated as follows:  \\nZ=𝛽1𝑥1+𝛽2𝑥2+...+𝛽𝑑𝑥𝑑 \\nS (𝛽)=𝛽𝑇𝜇1−𝛽𝑇𝜇2\\n𝛽𝑇𝐶𝛽                : Score function  ↓ \\nS (𝛽)=𝑍1−𝑍2\\n𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒 𝑜𝑓 𝑍 𝑤𝑖𝑡\\U0001d455𝑖𝑛 𝑔𝑟𝑜𝑢𝑝𝑠 \\nOn a given score function, the problem is to estimate the \\nlinear coefficients  of variables that maximize the score that  \\ncan be found as follows:  \\n𝛽=𝐶−1(𝜇1−𝜇2)            : Model coefficients  \\nC = 1\\n𝑛1+𝑛2(𝑛1𝐶1+𝑛2𝐶2): Pooled covariance matrix  \\nWhere  \\n𝛽          : Linear model coefficients  \\n𝐶1,𝐶2 : Covariance matrices  \\n𝜇1,𝜇2   : Mean vectors\"),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 1}, page_content='C = 1\\n𝑛1+𝑛2(𝑛1𝐶1+𝑛2𝐶2): Pooled covariance matrix  \\nWhere  \\n𝛽          : Linear model coefficients  \\n𝐶1,𝐶2 : Covariance matrices  \\n𝜇1,𝜇2   : Mean vectors  \\nA way of assessing the effectiveness of the discrimination \\nthus created is by calculating the Mahalanobis distance \\nbetween two groups formed by LDA. A distance of more than \\n3 means suggests that the two averages differ by more than 3 \\nstandard deviations which thus is an indicator of good \\nseparability  between the two  groups.  \\n𝛥2=𝛽𝑇(𝜇1−𝜇2) \\n𝛥∶ 𝑀𝑎\\U0001d455𝑎𝑙𝑎𝑛𝑜𝑏𝑖𝑠 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝑏𝑒𝑡𝑤𝑒𝑒𝑛 𝑡𝑤𝑜 𝑔𝑟𝑜𝑢𝑝𝑠 \\nFinally, a new point  (i.e. A person‟s profile) is classified into \\ndefault (C1) or not -default (C2) by projecting it onto the \\nmaximally separating di rection and classifying it in the class \\nC1 (default) if:  \\n𝛽𝑇(𝑥−(𝜇1+𝜇2\\n2))≥𝑙𝑜𝑔𝑝(𝐶1)\\n𝑝(𝐶2) \\n𝛽𝑇      : Coefficients vector  \\n𝑥         : Data vector  \\n𝜇1+𝜇2\\n2   : Mean vector  \\n𝑝(𝐶1)\\n𝑝(𝐶2)   : Class probability  \\nA sample graph  in LDA is as follow:'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 1}, page_content='𝛽𝑇(𝑥−(𝜇1+𝜇2\\n2))≥𝑙𝑜𝑔𝑝(𝐶1)\\n𝑝(𝐶2) \\n𝛽𝑇      : Coefficients vector  \\n𝑥         : Data vector  \\n𝜇1+𝜇2\\n2   : Mean vector  \\n𝑝(𝐶1)\\n𝑝(𝐶2)   : Class probability  \\nA sample graph  in LDA is as follow:  \\nof the M. The best split on these m is used to split t\\n \\n2.2 B. Random Forest Algorithm  \\nDecision Trees - \\nTree based learning algorithms like Decision Trees are \\nconsidered to be one of the best and mostly used in the \\ncategory of supervised learning methods. Tree based methods \\nencourage predictive models with stability, high accuracy and \\neasy of exploration. Tr ee based methods map the non -linear \\nrelationships with a good accuracy. This method breaks down \\nthe dataset into smaller and smaller subsets of data while in \\nthe same period an associated decision tree is developed in an \\nincremental manner. We thus get a t ree with decision nodes \\nand tree nodes. Decision trees can handle both categorical and \\nnumerical data. A sample Decision Tree is as follows:'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 2}, page_content='International Journal of Computer Applications (0975 – 8887)  \\nVolume 161 – No 11, March 2017  \\n3  \\nAfter the decision tree is ready for our database, we will \\ncreate a set of rules that defines the major objective  of our \\nproject i.e. knowing if the credit risk will be good or bad.  \\nAt the end, result of both the algorithms is compared to know \\nif regression or decision tree would prove efficient as \\ncompared to the other.  \\nRandom Forest Algorithm - \\nIn our project, unde r Decision Trees we apply Random Forest \\nalgorithm. Random Forest is a multifaceted machine learning \\ntechnique which is capable of performing both classification \\nand regression tasks. It also treats missing values, undertakes \\ndimensional reduction methods, outlier the values and other \\nessential steps at the data exploration stage.  It is an ensemble \\nlearning technique, where a group of weak models is  \\ncombined to form a powerful model. To classify a new object'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 2}, page_content='essential steps at the data exploration stage.  It is an ensemble \\nlearning technique, where a group of weak models is  \\ncombined to form a powerful model. To classify a new object \\nbased on attributes, each tree assigns a classific ation i.e. in \\nother words it “votes” for that particular class. The forest (i.e. \\nthe algorithm) chooses the classification having the most \\nvotes (over all the other trees in the forest) and in case of \\nregression, the algorithm takes average of different ou tputs.  \\nThe algorithm is as follows:  \\n1. Assume number of cases in the training set is N. \\nThen, sample of these N cases is taken at random \\nbut with replacement. This sample will be the \\ntraining set for growing the tree.  \\n2. If there are M input variables, a number m<M is \\nspecified such that at each node, m variables are \\nselected at random out he node. The value of m is \\nheld constant while we grow the forest.  \\n3. Each tree is grown to the largest extent possible and \\nno pruning is done.'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 2}, page_content='selected at random out he node. The value of m is \\nheld constant while we grow the forest.  \\n3. Each tree is grown to the largest extent possible and \\nno pruning is done.  \\n4. Predict new data by aggregating the  predictions of \\nthe n tree trees (i.e., majority votes for \\nclassification, average for regression).  \\n \\n2.3 Logistic Regression  \\nLogistic regression, developed by David Cox, is one of the \\nmost frequently used statistical model used in credit scoring. Logistic  regression  can be seen as a special case of the linear \\nmodel and analogous to linear regression. Logistic and linear \\nregression model differ in their outcome. The outcome of \\nlogistic regression is discrete rather than continuous.  \\nLogistic regression model s the relationship between \\nindependent variables and one or more independent  variables. \\nThe constraints the estimated probabilities to lie between 0 \\nand 1. Depending on the values of attributes (independent'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 2}, page_content='independent variables and one or more independent  variables. \\nThe constraints the estimated probabilities to lie between 0 \\nand 1. Depending on the values of attributes (independent \\nvariables), we will find the probability that the  dependent \\nvariable takes value 0 (default probability). It is commonly \\nused for prediction and forecasting. Profit, sales, diseases, \\nprobability of failure of a given process, could all be predicted \\nusing regression techniques.  \\nThe conditional probability  𝑃𝑟(𝑌=1| 𝑋=𝑥) is modeled as a \\nfunction of  𝑥 and 𝑌is a binary output variable.  It uses the \\nform  \\n𝑃(𝑥)=𝑒𝛽0 + 𝑥 · 𝛽 \\n1 +  𝑒𝛽0 + 𝑥 · 𝛽  \\nWe can also re -express the above equation as  \\n𝑙𝑜𝑔(𝑃(𝑥)\\n1 − 𝑃(𝑥)) = 𝛽0 + 𝑥⋅𝛽 \\nThe coefficients β are estimated using the maximum \\nlikelihood method. The solution of β0 + x · β = 0 gives  us the \\ndecision boundary separating the two predicted classes. The \\nlogistic function is a nonlinear function and this means that'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 2}, page_content='likelihood method. The solution of β0 + x · β = 0 gives  us the \\ndecision boundary separating the two predicted classes. The \\nlogistic function is a nonlinear function and this means that \\nthe probability that 𝑌=1 𝑜𝑟 𝑌=0is not constant with \\nconstant changes in the predictor variable 𝑋. \\n2.4 XGBoost  \\nXGBoost is an open source implementation of Gradient \\nBoosting Machines. It is a scalable and high  performance \\nmachine learning system for tree boosting and claims to run \\n10 times faster than existing solutions. It provides parallel tree \\nboosting that solve many data science problems in a fast and \\naccurate way. It also provides cache access patterns, data \\ncompression and sharing  for tree boosting.  \\nXGBoost is used for supervised  learning problems. The model \\nfor XGBoost is tree ensembles which consist of classification \\nand regression trees. Tree ensembles are also a model for \\nrandom forests. The difference between the two is how we'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 2}, page_content='for XGBoost is tree ensembles which consist of classification \\nand regression trees. Tree ensembles are also a model for \\nrandom forests. The difference between the two is how we \\ntrain them.   Consider an objective function  \\n𝑜𝑏𝑗 =  𝑙 (𝑦𝑖,𝑦𝑖 (𝑡))𝑛\\n𝑖 = 1 +  𝛺 (𝑓𝑖)𝑡\\n𝑖 =1   \\nHere 𝑙 is a differentiable convex loss function and  Ω is the \\nregularization term. This model is trained in an additive \\nmanner. We note the prediction value at step 𝑡 by 𝑦𝑖 (𝑡), so we \\nhave  \\n𝑦𝑖 (𝑡) =  𝑓𝑘 (𝑥𝑖)𝑡\\n𝑘 = 1 =𝑦𝑖 (𝑡−1)𝑓𝑡(𝑥𝑖)   \\n𝑜𝑏𝑗(𝑡) =  𝑙 (𝑦𝑖,𝑦𝑖 (𝑡)) 𝑛\\n𝑖 = 1+  𝛺 (𝑓𝑖)𝑡\\n𝑖 =1   \\n   =  𝑙 (𝑦𝑖,𝑦𝑖 (𝑡 −1) + 𝑓𝑡(𝑥𝑖)) +𝛺 (𝑓𝑡) +𝑐        𝑛\\n𝑖 = 1'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 3}, page_content='International Journal of Computer Applications (0975 – 8887)  \\nVolume 161 – No 11, March 2017  \\n4 Here 𝑐is constant. We take the Taylor expansion of the loss \\nfunction up to the second order.  \\n𝑜𝑏𝑗(𝑡)= 𝑙 (𝑦𝑖,𝑦𝑖 (𝑡 −1)+ 𝑔𝑖 𝑓𝑡(𝑥𝑖))+ 1\\n2\\U0001d455𝑖 𝑓𝑡2(𝑥𝑖) ]𝑛\\n𝑖 = 1\\n+𝛺 (𝑓𝑡)+𝑐 \\nWhere 𝑔𝑖 and \\U0001d455𝑖 are defined as,  \\n𝑔𝑖=𝛿𝑦  (𝑡−1)𝑙(𝑦𝑖, 𝑦𝑖 (𝑡 −1)) \\n\\U0001d455𝑖=𝛿2\\n𝑦  (𝑡−1)𝑙(𝑦𝑖,𝑦𝑖 (𝑡 −1)) \\n3. FIGURES/CAPTIONS  \\n \\nFig 1 Process Flow for Developing Scoring Model:  \\n \\nFig 2 System Architecture:  \\n4. CONCLUSION  \\nThe In this work, we have surveyed various machine learning \\nalgorithms with respect to credit scoring along with various \\nensemble techniques. In general, we have decided  to \\nimplement the model with four different machine -learning models for creating credit risk scorecards. Different analysis \\ntechniques will be used to find the accuracies of models.  \\n5. REFERENCES  \\n[1][Li, Xiao -Lin, and Yu Zhong. \"An overview of personal \\ncredit scoring: techniques and future work.\" (2012).'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 3}, page_content='techniques will be used to find the accuracies of models.  \\n5. REFERENCES  \\n[1][Li, Xiao -Lin, and Yu Zhong. \"An overview of personal \\ncredit scoring: techniques and future work.\" (2012).  \\n[2]West, Jarrod, and Maumita Bhattacharya. \"Some \\nExperimental Issues in Financial Fraud Mining.\" \\nProcedia Computer Science 80 (2016): 1734 -1744.  \\n[3]Yap, Bee Wah, Seng Huat Ong, and Nor Huselina \\nMohamed H usain. \"Using data mining to improve \\nassessment of creditworthiness via credit scoring \\nmodels.\" Expert Systems with Applications 38.10 \\n(2011): 13274 -13283.  \\n[4]Jayagopal, B. \"Applying Data Mining Techniques to Credit \\n[5]Hand, David J., and William E. Henley . \"Statistical \\nclassification methods in consumer credit scoring: a \\nreview.\" Journal of the Royal Statistical Society: Series \\nA (Statistics in Society) 160.3 (1997): 523 -541. \\n[6]James, Gareth, et al. An introduction to statistical learning . \\nVol. 6. New York: springer, 2013.  \\n[7]Kennedy, Kenneth. \"Credit Scoring Using Machine'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 3}, page_content='[6]James, Gareth, et al. An introduction to statistical learning . \\nVol. 6. New York: springer, 2013.  \\n[7]Kennedy, Kenneth. \"Credit Scoring Using Machine \\nLearning.\" (2013).  \\n [8]Khandani, Amir E., Adlar J. Kim, and Andrew W. Lo. \\n\"Consumer credit -risk models via machine -learning \\nalgorithms.\" Journa l of Banking & Finance 34.11 (2010): \\n2767 -2787.  \\n[9]Yap, Bee Wah, Seng Huat Ong, and Nor Huselina \\nMohamed Husain. \"Using data mining to improve \\nassessment of creditworthiness via credit scoring \\nmodels.\" Expert Systems with Applications  38.10 (2011): \\n13274 -13283. \\n[10]Siddiqi, Naeem. Credit risk scorecards: developing and \\nimplementing intelligent credit scoring . Vol. 3. John \\nWiley & Sons, 2012.  \\n[11]Hand, David J., and William E. Henley. \"Statistical \\nclassification methods in consumer credit scoring: a \\nreview.\" Journal of the Royal Statistical Society: Series \\nA (Statistics in Society)  160.3 (1997): 523 -541. \\n[12]Chen, Tianqi, and Tong He. \"xgboost: eXtreme Gradient'),\n",
       " Document(metadata={'source': 'data/1.pdf', 'page': 3}, page_content='review.\" Journal of the Royal Statistical Society: Series \\nA (Statistics in Society)  160.3 (1997): 523 -541. \\n[12]Chen, Tianqi, and Tong He. \"xgboost: eXtreme Gradient \\nBoosting.\" R package version 0.4 -2 (2015).  \\n[13]Breiman, Leo. \"Random forests.\" Machine learning  45.1 \\n(2001): 5 -32. \\n[14]Izenman, Alan Julian. \"Linear discriminant analysis.\" \\nModern Multivariate Statistical Techniques . Springer \\nNew York, 2013. 237 -280. \\n[15]Hosmer, David W., and Stanley Lemeshow. Multiple \\nlogistic regression . John Wiley & Sons, Inc., 2000.  \\n[16]Tianqi Chen and Carlos Guestrin. “XGBoost: A Scalable \\nTree Boosting System.”(2016).  \\n \\nIJCATM : www.ijcaonline.org'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 0}, page_content='A comparative study of discrimination methods for\\ncredit scoring\\n(Invited Paper)\\nHsiang-chun Chen\\nDepartment of Statistics\\nTexas A & M University\\nCollege Station, Texas, 77843-3143, USA\\nEmail: ahcchen@neo.tamu.eduYi-chin Chen\\nDepartment of Industrial and Systems Engineering\\nTexas A & M University\\nCollege Station, Texas, 77843-3131, USA\\nEmail: yichin@neo.tamu.edu\\nAbstract —Credit scoring has become an important manage-'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 0}, page_content='ment science issue as the credit industry has been experiencingenormous growth during the past few decades. Numerous popularclassiﬁcation methods (e.g. linear discriminant analysis, quadraticdiscriminant analysis, and logistic regression) have been appliedin credit scoring for years. Recently researchers proposed severalsophisticated and highly effective data mining techniques, such asSkew-normal discriminant analysis (SNDA), Skew-t discriminantanalysis (STDA), Stepwise discriminant analysis (SDA), Sparsediscriminant analysis (Sparse DA), Flexible discriminant analysis(FDA), and Mixture discriminant analysis (MDA). The objectiveof this study is to examine these recently proposed discriminationmethods for screening credit applicants. The performance ofvarious credit scoring models is evaluated by one real-worldcredit scoring dataset. The predictive ability of each credit scoringmodel is accessed by the total percentage of correctly classiﬁedcases (total PCC) and the bad rate among'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 0}, page_content='by one real-worldcredit scoring dataset. The predictive ability of each credit scoringmodel is accessed by the total percentage of correctly classiﬁedcases (total PCC) and the bad rate among accepts (BRA). Theresults show that SNDA, STDA, and SDA are outperformingtechniques for implementing credit scoring models.'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 0}, page_content='I. INTRODUCTION\\nCredit scoring is an evaluation for creditworthiness of a\\nperson based on this person’s past credit ﬁles. Credit scorescan not only be used for assessing potential risks but also helps\\nﬁnancial institutions discriminate between those applicants\\nwhom will repay a loan or card and decide whether to grant\\nor reject credit to applicants. The goal of constructing credit\\nscoring models is to classify credit applicants as good or badpayers. Financial institutions could determine who qualiﬁes for\\na loan, at what interest rate, and what credit limits in terms\\nof credit scores. The use of credit scoring prior to authorizingaccess or granting credit is an implementation of a trusted\\nsystem.\\nMany researchers have developed sophisticated credit scoring\\nmodels for credit applications on the basis of the appilcants\\ndata. The data of applicants consist of the individual behaviorsthat have been are observed over a period of time, such as'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 0}, page_content='models for credit applications on the basis of the appilcants\\ndata. The data of applicants consist of the individual behaviorsthat have been are observed over a period of time, such as\\ncredit history, purpose, gender or ages. A number of credit\\nscoring models have been published and focused on differentemphasizes. Researchers treated lending decisions as binary\\nclassiﬁcation problems. Discriminantion methods or regression\\nmethods have been applied to credit scoring problems for\\nyears. Durand(1941) used quadratic discriminant analysis to\\nclassify credit applicants as good or bad payers. Many non-statistical methods are also developed, e.g. decision trees and\\nNeural networks (NN). Data mining approaches are used in\\nestablishing credit scoring models as well. A lot of new\\nclassiﬁcatory algorithms have been researched nowadays. The\\nmost of banks use these models to score their applicants.\\nIn this study, some recently proposed discrimination methods'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 0}, page_content='classiﬁcatory algorithms have been researched nowadays. The\\nmost of banks use these models to score their applicants.\\nIn this study, some recently proposed discrimination methods\\nare used as tools for constructing classiﬁcation rules for credit\\nscoring. The remainder of the study is organized as follows.\\nWe present a theoretical introduction of various discriminantanalyses in section 2. A compar ison of these discriminant\\nanalyses introduced above is disc ussed. In section 3, several\\nevaluation methods of the discriminant analysis are provided.One real world dataset: German credit dataset from the UC\\nIrvine Machine Learning Repository (Asuncion and Newman,\\n2007) is introduced in section 4. Then, all discriminant anal-\\nyses discussed in section 2 are applied on the German Credit\\ndataset and their predictive performances are evaluated bythe total percentage of correctly classiﬁed cases (total PCC)\\nand the bad rate among accepts (BRA). Plots of the receiver'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 0}, page_content='dataset and their predictive performances are evaluated bythe total percentage of correctly classiﬁed cases (total PCC)\\nand the bad rate among accepts (BRA). Plots of the receiver\\noperating characteristics (ROC) curves are also provided toassess the predictive accuracy of these methods. Section 5\\nincludes with some conclusion and comparison.\\nII. THEORETICAL BACKGROUND\\nThe discriminant analysis is a popular technique for classi-\\nfying objects into one of two or more groups based on a set of\\nfeatures that describe the objects. It has been widely used inthe ﬁelds of economics, education, biology, engineering, etc.\\nLinear discriminant analysis (LDA) is proposed in 1936 by R.\\nA. Fisher. Numerous extended discrimination analyses have\\nbeen developed since then. In this section, we would like to\\ndiscuss some of these discrimination methods.\\nA. REVIEW OF LINEAR DISCRIMINANT ANALYSIS(LDA)\\nThe linear discriminant analysis (LDA) is a useful technique'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 0}, page_content='discuss some of these discrimination methods.\\nA. REVIEW OF LINEAR DISCRIMINANT ANALYSIS(LDA)\\nThe linear discriminant analysis (LDA) is a useful technique\\nfor constructing classiﬁcation rules. Given a data of mul-tivariate observation, suppose each observation comes from\\none of Kgroups which are denoted by Π\\n1,Π2,···,ΠK.\\nEach group consists of observations with the same or similar\\ncharacteristics. The d iscriminant analysis is a procedure for\\nconstructing a classiﬁcation rul e that maximizes the separation'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 1}, page_content='among all groups. The measurements on the observation are\\ncalled the feature variables. The simplest case in discriminant\\nanalysis is binary classiﬁcation problem, that is, there are onlytwo groups (K=2 ) . Credit scoring problem in this study is\\na binary classiﬁcation problem.\\nThe Bayes’ rule classiﬁcation is the most commonly used\\nclassiﬁcation strategy. The Bayess rule classiﬁer assigns the\\nobservation X=xtoΠ\\n1iff1(x)\\nf2(x)>π1\\nπ2,w h e r e πiis the\\nprior probability of Πiandfi(·)is the conditional multivariate\\nprobability density of Xfor the ith group, i=1,2, and assigns\\nX=xtoΠ2otherwise.\\nLinear discriminant analysis (Fisher, 1936) is a maximum\\nlikelihood classiﬁcation assuming each class is Gaussian\\ndistributed. Fisher created a linear discriminant function to\\nﬁnd maximum separation among the groups based upon all\\nknown measurements. In LDA, assume all measurements areindependent normally distributed with arbitrary mean vectors'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 1}, page_content='ﬁnd maximum separation among the groups based upon all\\nknown measurements. In LDA, assume all measurements areindependent normally distributed with arbitrary mean vectors\\nand the common covariance matrices. That is, the Kgroups\\nhave a common covariance matrix Σ\\nk=Σ for each K.\\nThen based on the Bayes’ rule classiﬁcation and derivation\\nof logarithm of the ratio of the two posterior probabilities,\\nFishers linear discriminant function (LDF) is derived from\\nL(x) = log(f1(x)π1\\nf2(x)π2). (1)\\nLDF is a linear combination of all input variables. The mea-\\nsurements of an observation are used to help collect necessary\\ninformation for prediction. Then we estimate the coefﬁcientsof LDF. It has been proven that the estimated coefﬁcients for\\nLDF are proportional to those for least square regression. For\\neach subject, the discriminant score is the posterior probability\\nthat the subject belongs to the second group given all measure-'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 1}, page_content='LDF are proportional to those for least square regression. For\\neach subject, the discriminant score is the posterior probability\\nthat the subject belongs to the second group given all measure-\\nments of the subject. If the discriminant score of the functionis less than or equal to the cutoff point, the case is classiﬁed\\ninto the ﬁrst group, or if above it is classiﬁed into the second\\ngroup.The aim of the discrimination analysis is forming a classiﬁ-\\ncation rule that maximizes the variance between groups but\\nminimizes the variance within groups. Thus solving the dis-\\ncriminant problems is equivalent to estimate the discriminant\\ndirection athat\\nmax\\naaBTa (2)\\nsubject to aWTa=1\\nwhere Bis the between-class covariance and Wis the within-\\nclass variance. This is called the Fishers criterion.\\nHowever, LDA is not appropriate in some situations. For\\nexample, when linear boundaries are insufﬁcient in separatingthe classes or a single prototype per class is insufﬁcient, LDA'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 1}, page_content='However, LDA is not appropriate in some situations. For\\nexample, when linear boundaries are insufﬁcient in separatingthe classes or a single prototype per class is insufﬁcient, LDA\\nfails. Various generalizations of LDA are developed to solve\\nthose difﬁculties.While the covariance matrices are unequal, the quadratic\\nclassiﬁcation rule developed. Se parate covariance matrices for\\neach class are estimated. This technique is called the Quadratic\\ndiscriminant analysis (QDA). The method for creating the\\ndiscriminant function is similar to what we do in LDA.However, QDA would fail when the covariance matrix is\\nsingular. Logistic regression is also widely used in classi-\\nﬁcation problems. Logistic discriminant analysis works welland it works without assumption of distribution. But logistic\\ndiscriminant analysis is less efﬁcient than LDA.\\nIn order to enhance the efﬁciency of the classiﬁcation rules,\\nseveral modiﬁed discrimination methods are proposed, e.g.'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 1}, page_content='discriminant analysis is less efﬁcient than LDA.\\nIn order to enhance the efﬁciency of the classiﬁcation rules,\\nseveral modiﬁed discrimination methods are proposed, e.g.\\nSkew-normal discriminant analysis (SNDA), Skew-t discrimi-nant analysis (STDA), Stepwise discriminant analysis (SDA),\\nSparse discriminant analysis (Sparse DA), Flexible discrim-\\ninant analysis (FDA), and Mixture discriminant analysis(MDA).\\nB. SKEW-NORMAL AND SKEW-T DISCRIMINANT ANALY-\\nSES\\nSkew-normal and Skew-t discriminant analyses are mod-\\niﬁed classiﬁcation techniques. In Skew-normal discriminant\\nanalysis (SNDA), instead of assuming normal distribution in\\nLDA, we take the conditional multivariate probability density\\nf\\n1(·)ofXfor the ﬁrst group to be Skew-normal (μ1,Σ,α)\\nand the conditional multivariate probability density f2(·)ofX\\nfor the second group to be Skew-normal (μ2,Σ,α). Among\\nsub-populations, the location parameters could be unequal\\nwhile the scale matrix and the skewness vector parameter keep'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 1}, page_content='for the second group to be Skew-normal (μ2,Σ,α). Among\\nsub-populations, the location parameters could be unequal\\nwhile the scale matrix and the skewness vector parameter keep\\nconstant. Similar to LDA, the discriminant function, L(x)=\\nlog(f1(x)π1\\nf2(x)π2), is a linear combination o f all measurements,\\nand the coefﬁcients can be estimated by maximum likelihood\\nestimation.Another good method for relaxing the restriction of equal scale\\nmatrix and the skewness vector parameter without inﬂating the\\nnumber of unknown parameter is assuming that the conditionalmultivariate probability densities of Xfor the ith group, f\\ni(·)\\n, are Skew-t distributed. Similarly, the coefﬁcients of the dis-\\ncriminant function could be estimated by maximum likelihood\\nestimation. The classiﬁcation rule is generally referred to as\\nSkew-t discriminant analysis (STDA).Due to the nesting of the parametric classes, the use of SN or\\nST distribution improves the classiﬁcation a lot. Though there'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 1}, page_content='Skew-t discriminant analysis (STDA).Due to the nesting of the parametric classes, the use of SN or\\nST distribution improves the classiﬁcation a lot. Though there\\nis one more parameter that needs to be estimated in STDAthan in SNDA, STDA leads to a visible improvement.\\nC. STEPWISE DISCRIMINANT ANALYSIS(SDA)\\nStepwise discriminant analysis (SDA) is also a widely used\\ntechnique for building multivariate classiﬁers. SDA tries to\\nﬁnd out the variables in the model that contribute most to the\\ndiscrimination between groups and builds the discrimination\\nstep-by-step. Similar to forwards stepwise variable selection,\\nall variables are reviewed at every step and checking the F-values which indicates the statistical signiﬁcance in the dis-\\ncrimination between groups. Then the variable that contributes\\nmost to the discrimination between groups is included. Keepadding variables until the p-value of the minimum F-value\\nexceeds the predetermined signiﬁcance level. Also, similar'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 1}, page_content='most to the discrimination between groups is included. Keepadding variables until the p-value of the minimum F-value\\nexceeds the predetermined signiﬁcance level. Also, similar\\nto backward elimination variabl e selection, all variables are\\nincluded in the model at each step and then the variable\\nthat contributes least to the prediction of group membership'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 2}, page_content='is eliminated. Keep deleting variables until the p-value of\\nthe maximum F-value does not exceed the predetermined\\nsigniﬁcance level.Costanza and Aﬁﬁ(1979) concluded that using a moderate sig-\\nniﬁcance level(0.1 to 0.25) as the predetermined signiﬁcance\\nlevel often comes with better performance.\\nA measure from this group is sometimes used in stepwise\\ndiscriminant analysis to determine whether adding an inde-pendent variable to the model will signiﬁcantly improve the\\ndiscrimination between th e dependent variables.\\nD. SPARSE DISCRIMINANT ANALYSIS(SPARSE DA)\\nSparse discriminant analysis is a method that performs\\nlinear discriminant analysis with a sparseness criterion. By\\nadding an L1 penalty terms constraint on the weights, Sparse\\nDA regularizes the LDA loss function.Similarly to the Fishers criterion (1), the sparse discriminant\\ncriterion is\\nmax\\naaBTa−λ1p∑\\ni=1|ai| (3)\\nsubject to aWT\\npa=1\\nwhere Bis the between-class covariance and Wpis the\\npenalized within- class variance.'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 2}, page_content='criterion is\\nmax\\naaBTa−λ1p∑\\ni=1|ai| (3)\\nsubject to aWT\\npa=1\\nwhere Bis the between-class covariance and Wpis the\\npenalized within- class variance.\\nThe classiﬁcation, feature sel ection, and dimension reduction\\nare conducted simultaneously when conducting the Sparse DA.\\nThus the sparse discriminant analysis is faster in computation\\nand has better classiﬁcation results.\\nE. FLEXIBLE DISCRIMINANT ANALYSIS(FDA)\\nFlexible discriminant analysis (Hastie, Tibshirani, and Buja,\\n1994) is developed to improve LDA. Since LDA can be\\nviewed as a linear regression problem, FDA replaces the linearregression with a nonparametr ic regression. A nonparametric\\nregression is ﬁtted to the response on all input variables. And\\nthe optimal scores are obtained by computing the eigenvec-\\ntors matrix. We assign scores to the classes such that the\\ntransformed class labels are optimally predicted by regressionon the measurement. Then update the model by replacing'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 2}, page_content='tors matrix. We assign scores to the classes such that the\\ntransformed class labels are optimally predicted by regressionon the measurement. Then update the model by replacing\\nthe optimal scores. The regression method used in optimal\\nscaling in this study is the polynomial regression. FDA permitsto use non-linear decision boundaries. FDA can be viewed\\nas the application of LDA on the matrix obtained with the\\nnonparametric regression and on the transformed class matrix.\\nF . MIXTURE DISCRIMINANT ANALYSIS(MDA)\\nMixture discriminant analysis (Hastie and Tibshirani, 1996)\\nis a generalization of LDA. It is developed based on the\\nmixture Gaussian models. MDA keeps the assumption of\\nequal covariance matrix in LDA, but extends the assumption\\nof single normal model in LDA to a mixture of normaldistribution. The mixture of normal distributions is used to\\nobtain the density estimation for each class.\\nf\\ni(x)=∑\\nkpikφ(x|μik,Σ) i=1,2 (4)where φ(·)is the density of normal distribution with mean μik'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 2}, page_content='obtain the density estimation for each class.\\nf\\ni(x)=∑\\nkpikφ(x|μik,Σ) i=1,2 (4)where φ(·)is the density of normal distribution with mean μik\\nand covariance matrix Σ,a n d∑\\nkpik=1.\\nEM algorithm is used to estimate the parameters (pik,μik,Σ)\\nin a mixture of normal distribution for each individual class.\\nDue to the ﬂexibility, MDA outperforms LDA.\\nIII. EV ALUATION\\nThe performances of crediting scoring methods are eval-\\nuated by the capability of distinguishing the good credit\\npopulation from the bad credit populations based on theapplicants past credit ﬁles. In these discrimination analyses,\\neach applicant will be assigned a discriminant score which\\nis the posterior probability that the applicant is a good payer\\ngiven the behavior measurements of this individual. Applicants\\nwith higher score usually have good credits while applicantswith lower score usually have bad credits. The selection of\\ncut-off point affect making the granting decisions, hence the'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 2}, page_content='with higher score usually have good credits while applicantswith lower score usually have bad credits. The selection of\\ncut-off point affect making the granting decisions, hence the\\nperformance for each discrimi nation method. Proﬁciency of\\neach classiﬁcation procedure becomes quite important espe-\\ncially when misclassiﬁcations are costly.\\nA. THE TOTAL PERCENTAGE OF CORRECTLY CLASSI-\\nFIED CASES(TOTAL PCC)\\nThe actual error rate (AER) is the probability that a classi-\\nﬁcation rule conducted on a given sample will misclassify a\\nfuture observation (Type I error or Type II error), while the\\ntotal percentage of correctly cla ssiﬁed cases (total PCC) is the\\nprobability of correctly classifying a future observation. The\\ntotal PCC measures how accurately a predictive model will\\nperform in practice, thus it is an evaluation of the precision\\nof a classiﬁcation rule.\\nThere are several nonparametric methods to estimate the totalPCC. The simplest method is the substitution method which'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 2}, page_content='of a classiﬁcation rule.\\nThere are several nonparametric methods to estimate the totalPCC. The simplest method is the substitution method which\\none applies the classiﬁcation rul e to the sample, then generates\\na classiﬁcation rule and summarizes results in the confusiontable. The apparent error rate (APER) is the observed error\\nrate which is deﬁned as the ratio of the total number of\\nmisclassiﬁed observations to the total number of observations.\\nHowever, the substitution method usually overestimates the\\nactual correctly classiﬁed rates.If the sample is large, one can divide the sample into the\\ntraining set and validation set. The classiﬁcation rule is\\ncreated using the training set, while the apparent error rateis determined using the validation set. The precision of the\\nclassiﬁcation rule will depend on the split. However, this\\nmethod is evidently waste of the precious data available\\nbecause of the omission of the va lidation set in the training'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 2}, page_content='classiﬁcation rule will depend on the split. However, this\\nmethod is evidently waste of the precious data available\\nbecause of the omission of the va lidation set in the training\\nstage.The leave-one-out cross-validation method (Lachenbruch and\\nMickey, 1968) is also used to estimate the actual error rate.\\nIn leave-one-out cross-validation, one take one observationfrom the data set as the validation data, use the remaining\\nobservations to form the classiﬁcation rule, and then classify\\nthe omitted observation. The procedure is repeated until each\\nobservation in the data set is used once as the validation data.\\nHowever, because of large number of repeats, leave-one-out'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 3}, page_content='cross-validation is usually computationally expensive.\\nIn this study, ﬁve-fold cross-validation is used. The original\\ndataset is randomly partitioned into ﬁve partitions. Of the ﬁvepartitions, one partition is retained as the validation set for\\ntesting the model, and the remaining four partitions are used\\nas training set. The cross-valid ation process is then repeated\\nﬁve times with each of the ﬁve partitions used exactly once\\nas the validation data. The advantage of this method is thatall observations are used for both training and validation, and\\neach observation is used for validation exactly once.\\nB. THE BAD RATE AMONG ACCEPTS(BRA)\\nThe bad rate among accepts (Hand, 2005) is the proportion\\nof the applicants who is with high credit score eventually turnout to be non-creditworthy. Hand (2005) suggests that BRA\\nis an appropriate measure of effectiveness in credit scoring.\\nThe BRA also measures the predictive ability over a particular'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 3}, page_content='is an appropriate measure of effectiveness in credit scoring.\\nThe BRA also measures the predictive ability over a particular\\nrange of cut-off points. The higher predictive ability, the lower\\nthe BRA values. In this study, the BRA will be examined overthe accept rate from .75 to .90.\\nC. THE RECEIVER OPERATING CHARACTERISTICS\\n(ROC) curve\\nA receiver operating characteristic (ROC) curve is a plot of\\nthe fraction of true positive rates (TPR) vs. the fraction of falsepositive rates (FPR) for all possible cutoff points. The ROC\\ncurve is more informative than a classiﬁcation table because\\nit summarizes predictive power for all possible cutoff points.\\nThe ROC curves can be used to compare the diagnostic\\nperformance of two or more laboratory or diagnostic tests. Anidea ROC curve would go through point (0,0)to point (0,1)\\nalong Y−axis and then through point (0,1)to point (1,1)\\nparallel X−axis , while a ROC curve of a random performance\\nusually has a concave s hape connecting point (0,0)and point'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 3}, page_content='along Y−axis and then through point (0,1)to point (1,1)\\nparallel X−axis , while a ROC curve of a random performance\\nusually has a concave s hape connecting point (0,0)and point\\n(1,1). Thus the outperformed discrimination method usually\\ngives a steeper concave curve. The area under the ROC curve\\nis identical to the concordance index which is a measure of\\npredictive power and it is independent of the cutoff points.\\nIV . EMPIRICAL ANALYSIS\\nTo compare these discrimination methods, we apply them\\non one real-world credit scoring dataset in this study. This datais from the UCI Machine Learning Repository (Asuncion and\\nNewman, 2007).\\nA. DATA SAMPLES\\nThe German credit dataset consists of 1000 past credit\\napplicants, of whom 700 applicants were creditworthy appli-cants and 300 applicants were non-creditworthy applicants.\\nFor each applicant, twenty attr ibutes are used as predictors,\\nwhich include the applicants ag e, sex, credit history, credit'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 3}, page_content='For each applicant, twenty attr ibutes are used as predictors,\\nwhich include the applicants ag e, sex, credit history, credit\\namount, number of existing credits at this bank, etc. Thirteen\\nof these predictors are categorical and seven are quantitative.TABLE I\\nTOTAL PCC OF THE AUSTRALIAN CREDIT DATASET FOR THE\\nSNDA, STDA, SDA, S PARSE DA, FDA AND MDA.\\nMethod Total Good Bad\\nSNDA .8425 .8869 .7060\\nSTDA .8375 .8860 .7344\\nSDA .7330 .8173 .6736\\nSPARSE DA .7450 .7461 .7142\\nFDA .7750 .8054 .6863\\nMDA .7984 .8107 .5859\\nTABLE II\\nBRA OF FOR THE GERMAN CREDIT DATASET FOR THE LDA,\\nSNDA, STDA, SDA, S PARSE DA, FDA AND MDA.\\nAccept Rate\\nMethod 75 80 85 90\\nSNDA .1267 .1456 .1796 .2123\\nSTDA .1151 .1402 .1667 .2022\\nSDA .1773 .1988 .2259 .2478\\nSPARSE DA .2633 .2688 .2706 .2833\\nFDA .2026 .2113 .2341 .2522\\nMDA .2013 .2175 .2353 .2556\\nB. RESULTS\\nThe discrimination methods discussed above are applied on\\nthe German Credit dataset and ﬁve -fold cross validation is used'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 3}, page_content='FDA .2026 .2113 .2341 .2522\\nMDA .2013 .2175 .2353 .2556\\nB. RESULTS\\nThe discrimination methods discussed above are applied on\\nthe German Credit dataset and ﬁve -fold cross validation is used\\nto evaluate their performances. These methods are assessed bytheir total PCC , BRA and ROC curves.\\nFor the German credit dataset, the total PCC for each method\\nare listed in Table 1. The percen tage of correctly classiﬁed\\ngood risks and percentage of correctly classiﬁed bad risks\\nare also provided in Table 1. The results indicate that onbasis of the total PCC, the SDA outperform Sparse DA, FDA\\nand MDA while the SNDA and STDA outperform all other\\ndiscrimination methods.\\nTable 2 gives the BRA results for the German credit dataset.\\nThe results indicate that SNDA and STDA have higher pre-\\ndictive abilities than the othe r methods in the German credit\\ndataset because of the lower BRA values.'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 3}, page_content='The results indicate that SNDA and STDA have higher pre-\\ndictive abilities than the othe r methods in the German credit\\ndataset because of the lower BRA values.\\nThe ROC curves for the German credit dataset are plotted inFigure 1. The ROC curves for SNDA and STDA dominates the\\nother curves over most of the cut-off points. This suggests that\\nSNDA and STDA have higher predictive ability than SparseDA, FDA and MDA in the German credit dataset.\\nV . CONCLUSION\\nIn this study, several recently developed discrimination\\nmethods are applied for establishing credit scoring models.\\nThe performances of these techniques are evaluated by usingone real world data. From the results, the SNDA, STDA\\nand SDA outperformed the other discrimination methods on\\nGerman credit dataset. However, each discrimination methoddiscussed in this study may be robust in some speciﬁc models.\\nIt is impossible to conclude which method is the most effective\\ntechnique in general. The main contribution of this paper is to'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 3}, page_content='It is impossible to conclude which method is the most effective\\ntechnique in general. The main contribution of this paper is to\\nshow the comparison of these methods. On the basis of these\\nresults, it may conclude that SNDA, STDA and SDA may be'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 4}, page_content='Fig. 1. Receiver operating characteristic (ROC) curves of the German\\ncredit dataset for the LDA, SNDA, STDA, SDA, Sparse DA, FDAand MDA.\\nFalse positive rateTrue positive rate\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\\nFalse positive rateTrue positive rate\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\\nFalse positive rateTrue positive rate\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0ROC curves\\nFalse positive rateTrue positive rate\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\\nFalse positive rateTrue positive rate\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\\nFalse positive rateTrue positive rate\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0SNDA\\nSTDASDASparse DAFDAMDA\\ncompetitive methods for credit scoring.\\nREFERENCES\\n[1] A.C. Antonakis and M.E. Sfakianakis, Assessing naive Bayes as method\\nfor screening credit applicants , Journal of Applied Statistics, V ol. 36,\\nNo. 5, pp537-545, 2009.\\n[2] A. Agresti, An Introduction to Categorical Data Analysis , 2nd ed. John'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 4}, page_content='for screening credit applicants , Journal of Applied Statistics, V ol. 36,\\nNo. 5, pp537-545, 2009.\\n[2] A. Agresti, An Introduction to Categorical Data Analysis , 2nd ed. John\\nWiley and Sons Inc., 2007.\\n[3] A. Asuncion and D.J. Newman, UCI Machine Learning Repository,\\n2007. Available:http://www.ics.uci.edu/mlearn/MLRepository.html\\n[4] A. Azzalini and M. G. Genton, Robust likelihood methods based on the\\nskew-t and related distributions , International Statistical Review, V ol.\\n76,Issue 1, pp106-129, 2008.\\n[5] G.P. Cachon and P.H. Zipkin, Credit scoring with boosteddecision trees ,\\nMPRA Paper,University Library of Munich, Germany, No. 8034, 2008.\\n[6] L. Clemmensen, T. Hastie and B. Ersboll, Sparse discriminant analysis ,\\nTechnical report, IMM, Danish Technical University, 2008.\\n[7] J.N. Crook, D.B. Edelman and L.C. Thomas, Recent Developements in\\nConsumer Credit Risk Assessment , European Journal of Operation\\nResearch, V ol. 183, pp. 1447-1465 2007.'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 4}, page_content='[7] J.N. Crook, D.B. Edelman and L.C. Thomas, Recent Developements in\\nConsumer Credit Risk Assessment , European Journal of Operation\\nResearch, V ol. 183, pp. 1447-1465 2007.\\n[8] D. Durand, Risk Elements in Consumer Instalment Financing ,t h e\\nNatioal Bureau of Economic Research, 1941.\\n[9] S. Finlay, Credit Scoring for proﬁtability objectives , European Journal\\nof Operation Research, V ol. 202, pp. 528-537, 2010.\\n[10] D.J. Hand and W.E. Henley Statistical Classiﬁcation Methods in Con-\\nsumer Credit Scoring: A Review , J. Roy. Stat. Soc. A, V ol. 160, pp.\\n523-541, 1997.\\n[11] T. Hastie and R. Tibshirani, Discriminant Analysis by Gaussian Mix-\\ntures , Journal of the Royal Statistical Society, Series B, V ol. 58, No.\\n1, pp. 155-176, 1996.\\n[12] T. J. Hastie, R. Tibshirani and A. Buja, Flexible Discriminant Analysis\\nby Optimal Scoring , Journal of the American Statistical Association,\\nV ol. 89, pp. 1255-1270, 1993.\\n[13] A.J. Izenman, Modern Multivariate Statistical Techniques , Springer,'),\n",
       " Document(metadata={'source': 'data/2.pdf', 'page': 4}, page_content='by Optimal Scoring , Journal of the American Statistical Association,\\nV ol. 89, pp. 1255-1270, 1993.\\n[13] A.J. Izenman, Modern Multivariate Statistical Techniques , Springer,\\n2008.\\n[14] C. Reynes, R. Sabatier and N. Molinari, Choice of B-splines with free\\nparameters in the ﬂexible discriminant analysis context , Computational\\nStatistics and Data Analysis, V ol. 51, Issue 3, pp.1765-1778, 2006.[15] L.C. Thomas, J.N. Crook and D.B. Edelman, Credit Scoring and Its\\nApplications , Society for Industrial and Appplied Mathematics,\\nPhiladelphia, 2002.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 0}, page_content='Volume 7 • Issue 1 • 1000381 J Appl Computat Math, an open access journal\\nISSN: 2168-9679 Open Access Research ArticleJournal of  \\nApplied & Computational Mathematics Journal of Applied & Computational MathematicsISSN: 2168-9679Ghassempouri and Hoseini,  J Appl Computat Math  2017, 7:1\\nDOI: 10.4172/2168-9679.1000381Keywords: Credit scoring; Customer lifetime value; Discriminant \\nanalysis; RFM model\\nIntroduction\\nBreakthroughs in information and communication technology \\ntogether with the globalization of economy have created new challenges'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 0}, page_content='analysis; RFM model\\nIntroduction\\nBreakthroughs in information and communication technology \\ntogether with the globalization of economy have created new challenges \\nfor economic institutions in general and financial institutions and banks in particular [1-3]. Overcoming these challenges will not be possible without relying on scientific management. One of the foundations of scientific management is using mathematical models and soft computing to model and quantify phenomena [4-7]. Sound and accurate decision-making to resolve managerial problems would be impossible without the quantification of phenomena [8-10]. The \\nproblem of determining the customer’s creditworthiness is one of these \\nchallenges that demand addressing particularly following the prevalence of credit cards and selling methods that rely on the customer’s credit \\n[11-15]. The information on the individuals and businesses credit \\nscores is now a valuable asset produced and sold by certain credit'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 0}, page_content='[11-15]. The information on the individuals and businesses credit \\nscores is now a valuable asset produced and sold by certain credit \\nrating agencies [16-20]. Many institutions prefer to calculate their \\nown customers’ credit scores relying on the data already existing at the \\ninstitution [21-23]. This way, such institutions would avoid paying the \\ncosts pertaining to collecting extra data or buying entities’ credit scores from credit rating agencies. This study is attempting to meet this need \\nby presenting a plan that is practical, comprehensive, and feasible to \\nimplement at most institutions, and would also eliminate the need to \\npay extra costs [24,25].\\nLiterature and Basic Concepts\\nThe customer’s credit score, his lifetime value, and the default \\nrisk from his likelihood of non-repayment are among the primary'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 0}, page_content='requirements in formulating sales strategy and policy. The perception that long term relationship with a customer translates into more profitability has been held since long. Based on this concept is raised the subject of customer lifetime value (CLV, or LTV). In classic resources, CLV has been defined as the net worth gained from the customer as a result of having relationship with him, with a part or a cluster of customers as a continuing, consistent, and interactive flow. In models for calculating CLV, the total net income gained in the past is considered together with the customer’s current and future value. Questions that are brought up in calculating CLV are: What is the probability that the customer will buy the product or service again, as specified for each period? What is the approximate gross size of his purchase, as specified for each period? What is the cost of rendering services to the customer for his retention, as specified for each period? What is the cost of'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 0}, page_content='the approximate gross size of his purchase, as specified for each period? What is the cost of rendering services to the customer for his retention, as specified for each period? What is the cost of attracting a new customer (3). A variety of models have been introduced for calculating CLV, all of which in essence aim at predicting the customers’ future behavior by observing their past conduct. If we represent the customer’s transactions withθ, we will have past=f (θ) and future=f (θ). The aim of modeling is to determine the function f. Offering services in market is done based on a contract or without it and in continuous or discrete manner. Therefore, the relationship with the customer is considered in four different forms, and the modeling for the calculation of CLV is performed in correspondence with these four states. Studies show that, having the indices for the “recency” of contact and the “frequency” of purchase, it is possible to estimate CLV for each of these four states'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 0}, page_content='correspondence with these four states. Studies show that, having the indices for the “recency” of contact and the “frequency” of purchase, it is possible to estimate CLV for each of these four states (Hughes, 2006). Recency and frequency are represented with the triplet (x, t'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 0}, page_content='x, T). x Is the \\n*Corresponding author: Ghassempouri M, Islamic Azad University, Tehran, \\nTehran Province, Tehran, Iran, Tel: 981132290331; E-mail: st_m_ghasempouri@azad.ac.ir\\nReceived  October 09, 2017 ; Accepted December 20, 2017 ; Published  December \\n27, 2017Citation: Ghassempouri M, Hoseini SMS (2017) A Methodology for Calculating \\nCustomer Credit Score Based on Customer Lifetime Value Model. J Appl Computat \\nMath 7: 381. doi: 10.4172/2168-9679.1000381\\nCopyright: © 2017 Ghassempouri M, et al. This is an open-access article \\ndistributed under the terms of the Creative Commons Attribution License, which \\npermits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.A Methodology for Calculating Customer Credit Score Based on Customer \\nLifetime Value Model\\nGhassempouri M* and Hoseini SMS\\nIslamic Azad University, Tehran Province, Tehran, Iran\\nAbstract'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 0}, page_content='Lifetime Value Model\\nGhassempouri M* and Hoseini SMS\\nIslamic Azad University, Tehran Province, Tehran, Iran\\nAbstract\\nProper customer relationship management is among the facets that contribute to productivity at institutions. It \\nis a requirement for customer relationship managers, especially at financial and credit institutions and at banks,'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 0}, page_content='to calculate and determine the customer’s creditworthiness and credit score. The aim of this study is to present a solution for calculating the customers’ value and their credit score without incurring the costs for collecting extra information. The primary source of data for this study is operation system database. Due to differences among operation systems, a comprehensive schema of the database is defined first. Only conventional indices and variables have been used in this schema, so that the presented solution can be generalized and will be applicable to most economic institutions. The calculation of the customer’s creditworthiness is performed with regard to the three variables of the “recency” of contact, the “frequency” of transactions, and the “monetary” amount. The collected data is divided into the two populations of “good” and “bad” customers. Variables from those two populations that possess significant differences are identified using statistical methods. Those variables'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 0}, page_content='is divided into the two populations of “good” and “bad” customers. Variables from those two populations that possess significant differences are identified using statistical methods. Those variables are used in determining the customer’s credit score. Next, a solution is presented for comparing the efficiency of the models for the identification of the customer’s credit score. We will test and compare two statistical methods, the Logistic Regression model and the Fisher Discriminant Analysis, and two soft computing methods, the Multilayer Perception Network and the Vector Machine for determining the customer’s credit score. Additionally, a solution is offered for setting the number of layers and the number of neurons in the Multilayer Perception Network.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 1}, page_content='Citation: Ghassempouri M, Hoseini SMS (2017) A Methodology for Calculating Customer Credit Score Based on Customer Lifetime Value Model. J Appl \\nComputat Math 7: 381. doi: 10.4172/2168-9679.1000381Page 2 of 7\\nVolume 7 • Issue 1 • 1000381 J Appl Computat Math, an open access journal\\nISSN: 2168-9679 \\nnumber of transactions in interval (0,T], and tx is the time of transaction \\nx. The probability that a customer with the attribute (x, tx, T) will still be \\nactive in time T is represented by x P(active | x,  t ,  T) , and the number \\nof expected transactions in the future with mathematical expectation \\nis represented by ()x E [X T, T t |x, t , T] + . Authors have calculated this \\nexpression in their studies. The RFM (Recency, Frequency, Monetary)'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 1}, page_content='is represented by ()x E [X T, T t |x, t , T] + . Authors have calculated this \\nexpression in their studies. The RFM (Recency, Frequency, Monetary) \\nmodel was introduced by Cullinan [5]. The popularity of this model is due to its simplicity of use and the access to input data. This way, organizations can use their transactions in this model. In this model, the three factors of the regency of contact, the frequency of purchase or contact, and the monetary amount of purchase or exchange in a linear relation are the basis for calculation. Due to the popularity of this model, ways to further expand and solidify its theoretic foundations \\nare still under study. The present study will show that, having the three \\nfactors of the recency of contact, the frequency of purchase, and the \\nmonetary amount of purchase in a given time period, we can calculate \\nthe remaining CLV. Despite the popularity and reception that the RFM \\nmodel has enjoyed, it suffers from some shortcomings, too. Using the'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 1}, page_content='the remaining CLV. Despite the popularity and reception that the RFM \\nmodel has enjoyed, it suffers from some shortcomings, too. Using the \\nresults from this model may lead to weak decisions, it does not allocate \\nresources efficiently, and is not capable of predicting the customer’s \\nfuture status. Models other than RFM have also been introduced. These \\nmodels try to predict the customer’s future status and cash flow. Also, models have been presented that adopt a statistical approach. NBD (Negative Binomial Distribution) is one of these statistical models \\nthat work for continuous and repetitive purchase mode. Such models \\noffer a good statistical distribution for function. Rating customers’ \\ncreditworthiness is a measurement and evaluation system that uses the data from the past behavior and present specifications of the individuals that had received loans before. This system is in fact a tool \\nfor assessing and judging about the right amount of loan to be made'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 1}, page_content='for assessing and judging about the right amount of loan to be made \\nto the customer based on his personal data and his past credit history. \\nJust like any other approach in modeling, rating is only a simplification of a complex phenomenon in the real world, and at its best, it is just an approximation of the risk. The aims of rating are: controlling the \\nchoice of risk, converting the risk of non-repayment into appropriate \\ncost, managing credit reduction, assessing new loan plans, shortening \\nthe time required for approval, making sure about the soundness of the credit rules and their consistent execution, improving goal setting for amendment purposes, making financial policies proportionate \\nto specific risk limits, improving the request for the repayment of \\ninstallments with the aim to reduce the costs of delay in installment'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 1}, page_content='to specific risk limits, improving the request for the repayment of \\ninstallments with the aim to reduce the costs of delay in installment \\npayment. Rating customers’ creditworthiness and classifying the loans according to the customers’ credit score will result in a reduction in financial risk. To take control of the loan making process, the \\ntwo variables of minimum deposit and maximum loan allowed are \\nincorporated. This way, the amount of deposit and the maximum \\namount of permitted loan for a given customer will be determined after his credit score has been calculated. Methods like multivariate \\nregression for determining function f are presented in the relation\\nf:R→{Higher Score, Medium Score, Low Score}.\\nIn this equation, the R domain includes the two variables of the \\ncustomer’s income and the customer’s debt to income ratio. Different'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 1}, page_content='f:R→{Higher Score, Medium Score, Low Score}.\\nIn this equation, the R domain includes the two variables of the \\ncustomer’s income and the customer’s debt to income ratio. Different \\napproaches exist to customers’ credit rating. Some of these are: rating and scoring based on the customer’s creditworthiness evaluation, behavioral rating, rating and scoring for the receipt of installments, \\nrating and scoring customers to identify abuse and fraud. In the \\ncreditworthiness evaluation approach, the customer’s income or salary, \\nthe number of people under his sponsorship, his residence, and other such variables are the criteria for determining the customer’s credit. The behavioral approach relies on historical data from the past. For example, the customer’s financial activities, number of transactions, \\naccount balance, frequency and quantity of loan installment maturities, \\nand the lifetime of his account are input variables for predicting the'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 1}, page_content='account balance, frequency and quantity of loan installment maturities, \\nand the lifetime of his account are input variables for predicting the \\nprobability of his delinquency in paying the installments. In rating \\nfor installment receipt approach, the customer’s income and his delays in repayments are among the variables in decision making. \\nUsually, the goal is a timely and proper identification and action \\nproportionate to the customer’s credit and repayment delays. In the \\nabuse identification approach, the customers are rated based on \\ntheir probability of abuse. Different methods and algorithms have been used for rating. The modeling methods have been classified in \\nfour categories: statistical modeling, expert system based modeling, \\nmodeling with artificial intelligence techniques, and soft computing. \\nIn credit rating based on experts’ view, factors such as the customer’s \\nquality of loyalty, his ability to repay, the value of the collateral, loan'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 1}, page_content='In credit rating based on experts’ view, factors such as the customer’s \\nquality of loyalty, his ability to repay, the value of the collateral, loan \\npurpose or type, loan period, and its installment maturities are taken \\ninto consideration. This method is heavily dependent on the expert’s \\nexperience and opinion, and is also time consuming and prone to \\nerror. The efficiency of different rating methods is compared. The \\nresults indicate that soft computing approaches, especially the hybrid \\nones, are more accurate than statistical methods. Still, neural network \\nand soft computing methods lack clarity. In other words, they are \\nnot based on reasoning and are unable to precisely explain the vague \\nand uncertain if customer loss, customer life cycle, customer value, \\nand the policies for attracting and rating. To solve this problem, \\nit seems that studies are needed to develop the hybrid methods and \\nclustering methods. Another approach to achieve clarity is to merge'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 1}, page_content='it seems that studies are needed to develop the hybrid methods and \\nclustering methods. Another approach to achieve clarity is to merge \\nneural network with fuzzy system. Research on customer rating is \\nusually conducted by computer experts, hence generally lacking real \\nfinancial and market considerations. In fact, customer rating is aiming \\nat financial risk reduction, and the customer’s value and profitability \\nand his life time value are hardly accounted for. The practical outcomes \\nof using customer rating will be retaining customers are not taken into \\naccount. In most statistical methods, it is a required condition for the \\ndata distribution to be normal, but this condition is not necessary in \\nlogistic regression approach.\\nOne of the challenges faced with in rating is optimal default point'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 1}, page_content='data distribution to be normal, but this condition is not necessary in \\nlogistic regression approach.\\nOne of the challenges faced with in rating is optimal default point \\ndefinition for defining bad customer. Different factors such as laws and the economic institution’s requirements are involved in defining bad customer. According to the definition by the Basle Committee on Banking Supervision, a delay of more than 90 days in paying installments would render the customer as gone bad. At some other banks, delay in paying even one installment is enough basis to regard the customer as gone bad. Also, it is possible to simultaneously have several criteria for bad customer definition. This study tries to use random forest tree algorithm to obtain an optimal default for bad customer definition.\\nAims of the Study\\nAs shown in Literature Review, customer credit rating requires \\ncollecting personal and financial data on the customers. This is a costly'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 1}, page_content='Aims of the Study\\nAs shown in Literature Review, customer credit rating requires \\ncollecting personal and financial data on the customers. This is a costly \\nattempt which in some cases violates the customer’s legal rights, too. \\nStudies show that the customer’s legal rights are often neglected in \\ndata collection, the customer information is collected without their \\nconsent, and the collected data is usually more than what is needed for customer rating. Regarding their application and benefits, modeling \\nfor calculating the customer’s lifetime value and his creditworthiness \\nis a need even for small institutions. Unfortunately, data collection and \\nmodeling is costly. Therefore, it seems that studies need to be conducted \\nto come up with low cost models that would be able to cover the two'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 2}, page_content='Citation: Ghassempouri M, Hoseini SMS (2017) A Methodology for Calculating Customer Credit Score Based on Customer Lifetime Value Model. J Appl \\nComputat Math 7: 381. doi: 10.4172/2168-9679.1000381Page 3 of 7\\nVolume 7 • Issue 1 • 1000381 J Appl Computat Math, an open access journal\\nISSN: 2168-9679 \\n, , \\n11\\n2l\\nT\\nwb i\\niMin W W Cε ε\\n=+∑\\nDiscriminant analysis with fisher method\\nIn his studies, Fisher introduces a linear function for partitioning \\nof a set into two groups [11]. If X is representative of the set of \\nrandom variables Xi, so that X=(X1,X2,….Xp), and if we write the linear \\ncombination of random variables Xi, as the Y relation Y=w1X1+w2X2+…\\n.+w3Xp where p is the number of attributes, then one criterion for \\npartitioning is the Y mean for the two groups. If we represent these two groups by B and G one solution is to examine the difference between \\n()() | |E Y B and E Y G to find values for wi s that maximize this difference. \\nHowever, the condition 1i'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 2}, page_content='()() | |E Y B and E Y G to find values for wi s that maximize this difference. \\nHowever, the condition 1i\\niw=∑  should be met for the Y relation to \\nbe linear. The fundamental drawback of this method is that, despite the \\npresence of two different means, the data distribution may be in a way \\nthat it cannot be partitioned into two groups. To solve this problem, \\nFisher assumes that the sample variances of these two groups are \\nidentical. Then, he introduces the relation M as the separation criterion:\\n( )1\\n2      \\n    distance between sample means of two groupsM\\nsample variance of each group=                (1)\\nDividing the means distance by the square root of the variance, the \\nseparation criterion will become independent of the scale. So, changing Y into cY would not change the size of M.\\nSuppose that the sample means for the groups B and G are m\\nB \\nand mG, respectively. Also, S is the common and sample variance. The \\nseparating distance M will be:\\n()1\\n2.\\n..T GB\\nTmmMw'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 2}, page_content='Suppose that the sample means for the groups B and G are m\\nB \\nand mG, respectively. Also, S is the common and sample variance. The \\nseparating distance M will be:\\n()1\\n2.\\n..T GB\\nTmmMw\\nw Sw−=                        (2)\\nThe number of attributes in each group is equal to p. Therefore, mB, \\nmG,and w are vectors for p entries. And we have:\\n()T\\nG E Y|G .m w=                     (3)\\n()T\\nB E Y|B w.m=                     (4)\\n()TVar Y w.S.w=                      (5)\\nWe will put in relation M and will equal it to 0 after differentiating \\nwith respect to w:\\n()()( )()\\n()13\\n22.\\n0\\n.. ..T T\\nGBGB\\nTTw m m Swmm\\nwSw wSw−−−=                  (6)\\n()()()()( ) .. . .T TT\\nGB GB m m wSw Sw w m m−= −                   (7)\\nThe second derivative of the relation M is positive. Also, the \\nexpression \\n()( )..\\n.T\\nT\\nGBwSw\\nwm m− is a scalar. So, the minimum point will be \\nas:\\n( 1)( ( ))(TT\\nGB wS m m−−                       (8)\\nIn this approach, only the means and variance have been used.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 2}, page_content='Therefore, the existence of a normal distribution was not a requirement and this method will be applicable to any statistical distribution. In fact, w.x=c is the line perpendicular to the line that connects the means of the two groups. Then, w.x=c is deemed as the separating line. If c is defined as the cutoff point, then c should be chosen in the distance between the means of the two groups.subjects of customer LTV and credit rating using minimum attributes. So, the first question is how to collect the data required for modeling LTV and credit rating with minimum cost, and the second is what method is the right method for calculating the credit score and how we can identify this right method. Here, we first present a methodology for extracting data from the institution’s operation system database. We will show that the required data can be extracted without the need for extra costs such as filling out forms or customer interview and just by a general and conventional schema of the'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 2}, page_content='database. We will show that the required data can be extracted without the need for extra costs such as filling out forms or customer interview and just by a general and conventional schema of the operation system database. After calculating the customer value based on the classic RFM model, we will answer the question of whether this model can be used for customer credit rating too. Then, an appropriate methodology will be'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 2}, page_content='presented for comparing discriminant analysis methods in customer \\ncredit rating. Four discriminant analysis methods will be compared \\nand ordered according to their efficiency.\\nTheoretic Foundations of this Study\\nIn this study, statistical methods and soft computing are used for \\ncustomer credit rating. Of statistical methods, logistic regression and Fisher discriminant analysis, and of soft computing methods, multilayer perception network and support vector machines are examined and compared. Their mathematical foundations are briefly presented.\\nMultilayer perception network\\nIn multilayer perception network, each layer is made up of the \\nmatrix of weights W, bias vector b, and output vector Y. The output from the first hidden layer is defined as Y\\n1=f(W1.X+b1) and the output \\nfrom the second layer is defined as Y2=f(W2.X+b2). Function f(.) is'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 2}, page_content='1=f(W1.X+b1) and the output \\nfrom the second layer is defined as Y2=f(W2.X+b2). Function f(.) is \\nthe activation function, and is usually chosen of sigmoid or tangent sigmoid type. The chosen number of layers in most applications is three. If the input vector has m elements, each element from the input vector will feed forward n neurons with the activation function output. \\nThe activation function outputs will linearly mix with the weights and create the network output. Propagating the input vector to the output vector, the network will generate its own output. The generated output is compared with the real output. The difference between the two outputs is called error. The error value is back-propagated into the network as changes in weights and bias. If the error reduces to the specified value, the learning process will stop.\\nSupport vector machine\\nThis method separates with a hyperplane the data that have been'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 2}, page_content='Support vector machine\\nThis method separates with a hyperplane the data that have been \\ngrouped in binary form. The method of separating is in a way that the separation area between the hyper plane and the sample training data is maximized. In action, the data will not be completely separated by the hyper plane into two groups. Part of the data will be placed on the wrong side. Therefore, simultaneous with the increase in the separation area, the addition of the error from placing some of the data on the wrong side should be minimized. The C parameter as the error penalty coefficient in the model controls the relative cost of these objectives. The training data placed on the separation area or on the wrong side are called support vectors; as such vectors suffice for determining the separating hyper plane. In algebraic language, the ordered couples {(x\\ni,yi),i=1,…,l} are available as training data where xi \\nRn and yi ∈{1,-1}. The data have been classified into two sets by the yi'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 2}, page_content='i,yi),i=1,…,l} are available as training data where xi \\nRn and yi ∈{1,-1}. The data have been classified into two sets by the yi \\nvalue. The separation area stands between the two planes WTx +b=1 \\nand WTx +b=-1. In nonlinear situations, the input data are projected \\nonto another Euclidean space by the function zi=(xi). If we represent \\nthe error value for each input vector as εi, the support vector machine \\n(SVM) is the optimal answer for the following problem [24]:'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 3}, page_content='Citation: Ghassempouri M, Hoseini SMS (2017) A Methodology for Calculating Customer Credit Score Based on Customer Lifetime Value Model. J Appl \\nComputat Math 7: 381. doi: 10.4172/2168-9679.1000381Page 4 of 7\\nVolume 7 • Issue 1 • 1000381 J Appl Computat Math, an open access journal\\nISSN: 2168-9679 \\nDiscriminant analysis with logistic regression\\nThe weakness of the linear regression in discriminant analysis is the \\nlarge size of the difference in the range of the two sides of the regression \\nequation. If the left hand side of the relation is a function of probability, a better outcome will be achieved, because the likelihood that different points from the right hand side of the equation are projected onto one point will be reduced. Therefore, a function of p\\ni is written on the \\nleft hand side of the regression equation. In logistic regression, this function is recommended to be a logarithmic function, so that the regression equation will be written as eqn. (9):\\n0 1 1 2 2        1.1i'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 3}, page_content='0 1 1 2 2        1.1i\\ni i ip p for all i n\\niplog w x w x w x wp= …\\uf8eb\\uf8f6= + + +…+\\uf8ec\\uf8f7−\\uf8ed\\uf8f8                 (9)\\nSo, the probability value will be obtained via eqn. (10):\\nw.x\\nw.x1eiep=+                     (10)\\nIn general, if the vector values y in equation y=w0+wT\\nX assume only \\n0 and 1 so that we have yi∈{0,1}, then to discriminate and separate the \\ndata sets into two classes, the probabilities ()() 0| 1| P y x and P y x= =  \\nneed to be calculated.\\n()()() 011|\\n1tPy x\\nexp w w x= =\\n+ −+               (11)\\n()()( )\\n()( )0\\n00|\\n1T\\nTexp w w x\\nPy x\\nexp w w x−+\\n= =\\n+ −+              (12)\\nThe Methodology of the Study\\nThe steps for data collection methodology are: Creating a database'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 3}, page_content=\"subschema of the operation system. Define a diagram of the relation between the entities in the database. Extracting data with SQL commands and recording them in a table. Standardizing the variable values. Identifying and separating the independent and dependent variables. Identifying the distribution of the variables using the Anderson–Darling test. Identifying and determining the outliers. If the variable distribution is normal, the Dixon-Grubbs' test is used. If the variable distribution is not normal, the box plot is used. Determining the index for classifying the customers into two populations of “good” and “bad”. Analyzing and determining the significant difference between the two populations of “good” and “bad” customers. If the variable distribution is normal, the analysis of variance is used. If the variable distribution is not normal, the Kruskal-Wallis H test or the Mann–Whitney U test are used. Narrowing down the variables via clustering method and drawing dendrogram. The\"),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 3}, page_content='is used. If the variable distribution is not normal, the Kruskal-Wallis H test or the Mann–Whitney U test are used. Narrowing down the variables via clustering method and drawing dendrogram. The RFM model is developed and implemented. That is, the creditworthiness of each customer for each fiscal period is calculated. Also, to identify and predict the general trend of changes in customer creditworthiness, at institution level, the means for the creditworthiness of the customers for each fiscal period is calculated. Using time series and the customer’s creditworthiness for previous fiscal periods, we will be able to predict the customer’s creditworthiness for the coming fiscal periods. Therefore, the creditworthiness of the ith customer for the jth fiscal period is defined by V'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 3}, page_content='i,j in eqn. (13). In this \\nrelation, Ri,j is the recency of contact for ith customer in the jth fiscal \\nperiod and Fi,j and Mi,j are the number of transactions and the mean \\ndeposit of the \\n th customer for the jth fiscal period, respectively.\\nVij=Wr*Rij+Wf*Fij+Wm*Mij                 (13)\\nIf nj is the number of the customers of the institution in the jth \\nfiscal period, the mean creditworthiness of the customers for the jth fiscal period is defined by nj as in eqn. (14).\\nj\\n1\\njV\\nwn\\nij\\nn==∑i\\nj                     (14)\\nIf the number of past fiscal periods is m, and T is defined as a time \\nseries system for predicting, then Vi,m+1 is the creditworthiness of the \\nith customer and Vm+1 is the mean creditworthiness of the customers \\nfor the future fiscal period, so that these values are obtained via eqns. (15) and (16).\\nV\\ni,m+1=T(V1, V2, ….,Vm)                    (15)\\nVi,m+1=T(V1, V2, ….,Vm)                   (16)\\nThe variable Vi,m+1 represents the future creditworthiness of the ith'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 3}, page_content='V\\ni,m+1=T(V1, V2, ….,Vm)                    (15)\\nVi,m+1=T(V1, V2, ….,Vm)                   (16)\\nThe variable Vi,m+1 represents the future creditworthiness of the ith \\ncustomer. The variable Vm+1 shows the future creditworthiness of all \\ncustomers. For those customers who have received loans, and whose goodness and badness have been determined by examining the way they repay the loan installments, the values for V\\ni,j s are extracted and \\nthe VB and VG is obtained. So that:\\n{ } G i,jV V | i  1 1 Good j m = ∀= = …                (17)\\n{ } B i,jV V | i  0 1 Good j m = ∀= = …                (18)\\nIf the two populations VB and VG are different, then it is inferred'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 3}, page_content='that the histories of the good and bad customers differ. In case there is a significant difference between these two populations, there will be a credit scoring model based on the RFM model. To choose the right model for customer credit rating, the efficiency of the models is compared using the Kolmogorov index. The number of layers and neurons in the MLP model is determined by design of experiments. The steps for executing the credit scoring methodology are: Calculating the logistic regression equation. Calculating the Kolmogorov index, and the error for the logistic model. Calculating the Fisher equations. Calculating the Kolmogorov index, and the error for the Fisher model. Choosing the number of layers and neurons for the MLP model by design of experiments. Executing the steps for training and testing the neural network at different levels of the design of experiments. Determining the right number of levels and neurons using the regression equation of the design of experiments.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 3}, page_content='and testing the neural network at different levels of the design of experiments. Determining the right number of levels and neurons using the regression equation of the design of experiments. Executing the steps for training and testing for the right neural network model. Calculating the Kolmogorov index and the error for the neural network model. Calculating the discriminant equation for the vector machine method. Calculating the Kolmogorov index for the vector machine model. Comparing the means of the Kolmogorov index for the models using the analysis of variance and determining the most appropriate model.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 3}, page_content='Implementation and Case Study\\nThis case study is conducted in financial and credit institutions \\nenvironment. The statistical population for this study has been extracted from the databases of four financial institutions. In the operation system database of the institution, the subschema with entity diagram is defined as Figure 1. The savings account specifications, the loan specifications, and the account transactions are stored in the ACC table, the LOAN table, and the DETAILS table, respectively. There is one loan for each customer and there are some transactions for each deposit account or loan account. Data are extracted from this subschema using the SQL language.\\nAfter standardization and elimination of the out-of-range data, \\nthe result is stored in a table. About 70% of the data will be used in building, and the remaining 30% will be incorporated in testing the models. The statistic of the tests is calculated for the study variables'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 4}, page_content='Citation: Ghassempouri M, Hoseini SMS (2017) A Methodology for Calculating Customer Credit Score Based on Customer Lifetime Value Model. J Appl \\nComputat Math 7: 381. doi: 10.4172/2168-9679.1000381Page 5 of 7\\nVolume 7 • Issue 1 • 1000381 J Appl Computat Math, an open access journal\\nISSN: 2168-9679 \\nusing the Minitab software. In case the statistic of the Anderson test'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 4}, page_content='Volume 7 • Issue 1 • 1000381 J Appl Computat Math, an open access journal\\nISSN: 2168-9679 \\nusing the Minitab software. In case the statistic of the Anderson test \\nis above 0.576, distribution of data is not normal and zero hypothesis at the significance level of α=15% rejects. The results indicate that the distribution of the variables is not normal. To analyze the difference between the two populations, the Kruskal-Wallis H test is conducted. The Good variable, as the factor in the test assumes 1 and 0 values for good customers with no delay in repayment and bad customers, respectively. In Table 1, there are significant differences in the study variables for good and bad customers. We narrow down the number of variables through clustering. Here, correlation coefficient is defined as the criterion of similarity. Therefore, the entry (i,j) in the matrix of distances will be defined by relation \\n1 ij ij dr= − . With a similarity'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 4}, page_content='1 ij ij dr= − . With a similarity \\nsize of 70%, the AMT and PART variables go into the same cluster. Therefore, one of them can be eliminated.\\nThe execution of the proposed methodology for data collection and \\nextraction resulted in the applicability of the variables in rows 1 to 7 of Table 1 for determining the customer value and his credit score. The calculation of CLV is done using eqn. (14). Now, the good and bad customer populations with the CLV mean index are compared during the fiscal periods. The one-way analysis of variance test shows that the two populations have significant differences.\\nNow, with this difference, the RFM variables can be used to \\ndetermine the customer’s credit score. Four discriminant approaches \\nwill be discussed and their efficiencies will be compared. Each of these'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 4}, page_content='determine the customer’s credit score. Four discriminant approaches \\nwill be discussed and their efficiencies will be compared. Each of these \\nmethods is tested several times using different data. Their indices of efficiency will be compared, and the best method will be selected. The output from the model is the customer’s credit score. Good or bad customer is defined by cutoff point. If we show the cutoff point with s , and the obtained probability for the jth customer as P\\ni, then we have:\\nPi≤s→{customer i is Bad}                 (19)\\nPi>s→{customer i is Good}                  (20)\\nWe show the model’s efficiency index with Kolmogorov-Smirnov \\n(KS) in eqn. (21). The KS efficiency index is in fact the maximum distance between the two probabilities.\\nGB KS Max P P= −               (21)\\nIn this relation, PG is the correct identification of a good customer \\nand PB is the correct identification of a bad customer. These probabilities'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 4}, page_content='GB KS Max P P= −               (21)\\nIn this relation, PG is the correct identification of a good customer \\nand PB is the correct identification of a bad customer. These probabilities \\nwill change as the cutoff point changes. The logistic regression equation is calculated using Minitab software. As seen in eqn. (22), the R and F coefficients are positive and the M coefficient is negative. The goodness probability of the customer is obtained by eqn. (23).1.8296 0.002716R 0.05867F 0.02009M y= + +−′                (22)\\n()()\\n()()exp11yPexp y=+′\\n′                (23)\\nIn MATLAB environment, the efficiency index is calculated. To \\nmake sure about the soundness of the model performance and also to compare the efficiency of the models, the model efficiency index for 9 datasets is obtained. The results from the program execution are presented in Table 2.\\nThe discriminant analysis with Fisher method is executed in SPSS'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 4}, page_content='The discriminant analysis with Fisher method is executed in SPSS \\nenvironment. The Fisher linear functions are presented in eqns. (24 and 25) for bad and good customers, respectively.\\ny\\nB=-3.231+0.072R-0.023F+0.103M for Good=0               (24)\\nyG=-3.566+0.076R+0.013F+0.08M for Good=1               (25)\\nThe discriminant function is selected as y=min (yB, yG). The \\nefficiency index for 9 datasets is calculated and recorded in Table 3.\\nThe multilayer perception neural network is implemented \\nusing MATLAB software. The network is of feed forward type with Hyperbolic tangent activation function and updating network weights as error back propagation with Levenberg-Marquart (LM) algorithm. To select the number of hidden layers and the number of neurons in each layer, design of experiments executed in Minitab environment with two factors in two levels. The regression equation of the test is obtained as eqn. (26).\\nKS=76.18+0.695*Neurons+6.69*Layers-0.339*Neurons*Layers    (26).'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 4}, page_content='KS=76.18+0.695*Neurons+6.69*Layers-0.339*Neurons*Layers    (26).\\nTo determine an appropriate model, we use regression eqn. (26). \\nThe conclusion is that a model with one hidden layer and 20 neurons is \\nbetter. Test result is recorded in Table 4.\\nTo execute the algorithm for the vector machine, the model in \\nquestion needs to be converted into a quadratic equation. To that end, \\ngood and bad customers are defined with indices 1 and -1, respectively. \\nVector Y shows the good and bad customers with these indices. Then, \\nthe good and bad customers’ data are placed in Class A and Class B, \\nrespectively. The discriminant equation is obtained as eqn. (27).\\nY=w\\n1R+w2F+w3M+b=212R+124F+256M-10046              (27)\\nThe Kolmogorov indices are calculated for 9 test samples and \\nare recorded in Table 5. The test results are compared using one-way'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 4}, page_content='Y=w\\n1R+w2F+w3M+b=212R+124F+256M-10046              (27)\\nThe Kolmogorov indices are calculated for 9 test samples and \\nare recorded in Table 5. The test results are compared using one-way \\nanalysis of variance. The Tukey method has been chosen for testing the means difference. In comparison to other methods, the neural network method shows a higher efficiency.\\nFigure 1: Subschema of Database.Difference Between Variables in Two Populations , Good and Bad Customers\\nRow Variable Description Mean for \\ngoodMean for \\nbadH statistic \\nvalue\\n1 R Recency of contact 77.82 37.56 254.23\\n2NUMPAID Number of paid \\ninstallments7 12 160.44\\n3LIFETIME Duration of \\ncooperation10 8 60.46\\n4 F Number of \\ntransactions1.170 1.110 55.80\\n5 M Account balance 1.030 1.030 19.32\\n6 AGE Age 43 39 17.98\\n7 AMT Loan amount 5,000,000 8,000,000 14.45\\n8 PART Installment amount 250,000 400,000 12.26\\nTable 1: Difference between variables in two populations, good and bad, customers.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 5}, page_content='Citation: Ghassempouri M, Hoseini SMS (2017) A Methodology for Calculating Customer Credit Score Based on Customer Lifetime Value Model. J Appl \\nComputat Math 7: 381. doi: 10.4172/2168-9679.1000381Page 6 of 7\\nVolume 7 • Issue 1 • 1000381 J Appl Computat Math, an open access journal\\nISSN: 2168-9679 \\nNo significant difference in efficiency is observed among the \\nlogistic, Fisher, and vector machine methods. Despite the superiority of \\nthe neural network method, as seen in Figure 2, the logistic regression and Fischer methods are more stable against different data .\\nResults of the Study\\nExecuting the data collection methodology, statistical analysis, and \\ncustomer value calculation, the following results have been obtained:\\n• Extraction and identification of the right variables for \\ndetermining the customer value and his credit score from the \\noperation system database without incurring extra costs for \\ncollecting data from the customers.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 5}, page_content='determining the customer value and his credit score from the \\noperation system database without incurring extra costs for \\ncollecting data from the customers.\\n• Making sure about the existence of significant difference \\nbetween the two populations of bad and good customers based \\non the obtained variables.\\n• Calculating the customer value and determining the trend of changes in their value.\\n• Predicting the customer value for future fiscal periods.\\n• Identifying significant difference in customer value between the two populations of bad and good customers.\\nExecuting the methodology for customer credit rating, the Efficiency Indexes for Logistic Model\\nTest No. 1 2 3 4 5 6 7 8 9\\nKS 90.35 90.04 90.09 89.52 90.10 89.51 90.11 90.15 89.93\\nTable 2: Efficiency indexes for logistic model.\\nEfficiency Indexes for Logistic Model\\nTest No. 1 2 3 4 5 6 7 8 9\\nKS 89.42 90.30 89.98 89.88 89.86 90.23 90.24 89.94 90.60\\nTable 3: Efficiency indexes for fisher model.\\nEfficiency Indexes for Logistic Model'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 5}, page_content='Test No. 1 2 3 4 5 6 7 8 9\\nKS 89.42 90.30 89.98 89.88 89.86 90.23 90.24 89.94 90.60\\nTable 3: Efficiency indexes for fisher model.\\nEfficiency Indexes for Logistic Model\\nTest No. 1 2 3 4 5 6 7 8 9\\nKS 90.0532 89.7793 90.3263 89.9628 89.8013 90.2519 89.8439 90.157 89.7775\\nTable 4: Efficiency indexes for MLP model.\\nEfficiency Indexes for Logistic Model\\nTest No. 1 2 3 4 5 6 7 8 9\\nKS 89.7775 90.157 89.8439 90.2519 89.8013 89.9628 90.3263 89.7793 90.0532\\nTable 5: Efficiency indexes for support vector machine model.\\nFigure 2: Logistic regression and Fischer methods.following results were obtained in addition to presenting a method for \\ncomparing and selecting the rating model:\\n• If the multilayer perception neural network is set correctly, this model will be more efficient than other models in rating.\\n• Design of experiments can be used to set the number of hidden layers and the number of neurons in the multilayer perception model.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 5}, page_content='• Design of experiments can be used to set the number of hidden layers and the number of neurons in the multilayer perception model.\\n• The results from testing the statistical models show that this method has less variance compared to soft computing methods.\\nIn this study, a method was proposed for calculating the customer \\nscore based on the customer value model. We also showed how the efficiency of the models are evaluated and compared. It is evident that \\nmanagerial considerations will be influential in the final decision. \\nConsidering the fact that statistical methods are explainable and \\nhave less variance in the output, despite the superior efficiency of the \\nmultilayer perception model, statistical methods may be preferred.\\nReferences\\n1. Andrea B, Marisa B (2007) Credit Scoring for Microenterprise Lenders. Microenterprise Fund for Innovation .\\n2. Berger PD, Nasr NJ (1998) Customer lifetime value: Marketing models and Applications. Journal of Interactive Marketing.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 5}, page_content='2. Berger PD, Nasr NJ (1998) Customer lifetime value: Marketing models and Applications. Journal of Interactive Marketing.\\n3. Buttle Francis (2009) Customer Relationship Management. Elsevier.\\n4. Ricahrd C, Jiang W (1999) A Stochastic RFM Model . Journal of Interactive Marketing 13: 2-12.\\n5. Cullinan GJ (1978) Picking them by their batting averages: Recency-frequency-\\nmonetary .Marketing Association.\\n6. Hosmer WD, Lemeshow S, Sturdivant XR (2013) Logistic Regression Models for the Analysis of Correlated Data . Applied Logistic Regression.\\n7. Ehrenberg ASC (1959) The Pattern of Consumer Purchases. Journal of the Royal Statistical Society. Series C 8: 26-41.\\n8. Liran E, Mark J, Jonathan L (2013) Then impact of credit scoring on consumer lending. The RAND Journal of Economics 44: 249-274.\\n9. Peter F, Hardie B (2005) The Value of Simple Models in New Product Forecasting and Custome-Base Analysis. Applied Stochastic Models in \\nBusiness and Industry .'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 5}, page_content='9. Peter F, Hardie B (2005) The Value of Simple Models in New Product Forecasting and Custome-Base Analysis. Applied Stochastic Models in \\nBusiness and Industry .\\n10. FDIC (2007) Scoring and Modeling : Division of Supervison and Consumer \\nProtection.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 6}, page_content='Citation: Ghassempouri M, Hoseini SMS (2017) A Methodology for Calculating Customer Credit Score Based on Customer Lifetime Value Model. J Appl \\nComputat Math 7: 381. doi: 10.4172/2168-9679.1000381Page 7 of 7\\nVolume 7 • Issue 1 • 1000381 J Appl Computat Math, an open access journal\\nISSN: 2168-9679 \\n11. Fisher RA (1936) The use of multiple measurments in taxonomic problems. \\nAnals of Human Genetics, pp. 179-188.\\n12. Alfred F (2014) Acquiring Profitable Customers with Credit Scoring Models . \\nCapital Service.\\n13. Terry H (2013) Default definition selection for credit scoring . Artificial Intelligence Research.\\n14. Arthur MH (2006) Strategic Database Marketing .\\n15. Huseyin I, Bora A (2010) A comparisson of data mining techniques for credit \\nscoring in banking: A managerial perspective. Journal of Bussiness Economics \\nand Management 10: 233-240 .\\n16. Meike K, Barbara K, Meints M (2008) Profiling of Customers and Consumers-\\nCustomers Loyalty Programs and Practices.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 6}, page_content='and Management 10: 233-240 .\\n16. Meike K, Barbara K, Meints M (2008) Profiling of Customers and Consumers-\\nCustomers Loyalty Programs and Practices.\\n17. Adel L (2008) Credit Scoring Models Using Soft Computing Methods: A Survey The International Arab Journal of Information Technology 7.\\n18. Miglautsch J (2002) Application of RFM principles: What to do with 1-1-1 customers? Journal of Database Marketing & Customer Strategy Management 9: 319-324.19. Reichheld FF, Sasser WE (1990) Zero defections: Quality comes to services 68: 105-111.\\n20. Sadatrasoul SM, Gholamian  M, Siami M, Hajimohammadi Z (2013) Credit \\nscoring in bank and financial institution via data mining techniques a literature \\nreview. Journal of AI and Data Mining 1: 119-129.\\n21. David CS, Robert P (1994) An Industrial Purchase Process Application . Marketing Science.\\n22. Nazmul S , Hojjat A (2013) Computational Intelligence Synergies of Fuzzy Logic, Neural Networks and Evolutionary Computing.'),\n",
       " Document(metadata={'source': 'data/3.pdf', 'page': 6}, page_content='22. Nazmul S , Hojjat A (2013) Computational Intelligence Synergies of Fuzzy Logic, Neural Networks and Evolutionary Computing.\\n23. Lyn CT, David BE, Jonathan NC (2002) Credit Scoring and Its Application: SIAM.\\n24. Vapnik V (1998) Statistical Learing Theory. IEEE Transactions on Neural Networks 10: 988-999\\n25. Zhong Y, Xiao-LL (2012) An Overview of Personal Credit Scoring Techniques \\nand Future Work. International Journal of Intelligence Science 2: 181-189.'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 0}, page_content='Research Article\\nBig Data Credit Report in Credit Risk Management of\\nConsumer Finance\\nLu Gao1and Jian Xiao2\\n1School of Business, Shandong University, Weihai 264209, China\\n2School of Business, Beijing International Studies University, Beijing 100024, China\\nCorrespondence should be addressed to Jian Xiao; xiaojian@bisu.edu.cn\\nReceived 14 April 2021; Revised 9 May 2021; Accepted 22 May 2021; Published 16 June 2021\\nAcademic Editor:\\nWenqing Wu\\nCopyright © 2021 Lu Gao and Jian Xiao. This is an open access article distributed under the Creative Commons Attribution\\nLicense, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is\\nproperly cited.\\nTraditional consumer ﬁnance is a modern ﬁnancial service method that provides consumer loans to consumers of all classes. With\\nthe gradual improvement of China’ s credit reporting system, big data credit reporting has eﬀ ectively made up for the lack of'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 0}, page_content='the gradual improvement of China’ s credit reporting system, big data credit reporting has eﬀ ectively made up for the lack of\\ntraditional credit reporting and has been widely used in the consumer ﬁnance industry. In this context, the in-depth analysis of\\nthe speci ﬁc application of big data credit reporting in the credit risk management of consumer ﬁnance and the strengthening of\\nthe research on the application of big data credit reporting in the credit risk management of consumer ﬁnance are urgently\\nneeded to be resolved in the economic and ﬁnancial theoretical and practical circles ’problem. This article mainly studies the\\nresearch on credit risk management of consumer ﬁnance by big data. The experimental results of this paper show that the\\nmodel has a good forecasting ability, can distinguish between normal loan customers and default loan customers, and is suitable'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 0}, page_content='model has a good forecasting ability, can distinguish between normal loan customers and default loan customers, and is suitable\\nfor practical personal credit risk control business. The prediction accuracy of the default model of the fusion model is 97.14%,\\nand the default rate corresponding to the actual business is 2.86%. By combining the risk items such as the blacklist and gray list\\nin the Internet ﬁnance industry, the bad debt rate and illegal usury can be well controlled to meet industry supervision.\\n1. Introduction\\nPersonal credit evaluation is an important evaluation stan-\\ndard in personal credit reporting, and it has gradually entered\\nthe stage of market-oriented development. However, for a\\nlong period of time, China has relied on the personal creditreport of the personal credit center of the central bank asthe standard, with the rise of big data.\\nThere are many studies of personal credit risk assessment'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 0}, page_content='There are many studies of personal credit risk assessment\\nnationally and internationally, including studies of multi-source data, Internet data, and Internet behavior data, butfew studies exist to assess credit risk for personal big data.In theory, this research can enrich and perfect the theoreticalsystem of personal credit risk assessment [1, 2]. It will helpimplement China ’s inclusive ﬁnancial strategy, lower the\\nthreshold for ﬁnancial services, bene ﬁt more people, eradi-\\ncate poverty, and achieve social equity. At the same time, ithas a certain reference signi ﬁcance for the application of\\nbig data risk control in the industry [3, 4]. In terms of inter-pretability, new big data algorithms can evaluate the impor-tance of statistical indicators, and statistical indicators withthe highest comprehensive ranking of statistical indicatorshave better interpretability. On the whole, some big data\\nalgorithms are excellent in accuracy and stability and can'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 0}, page_content='algorithms are excellent in accuracy and stability and can\\nbe used as a strategic reserve for my country ’s new generation\\nof credit risk assessment models.\\nIn theory, Morris and Shin decompose bank credit risk\\ninto bankruptcy and liquidity risk and de ﬁne liquidity risk\\nas the possibility of reverse bankruptcy caused by the bank’ s\\noperation for free [5]. The “liquidity ratio ”(that is, the ratio\\nof cash to current liabilities) on the balance sheet has beenshown to reduce liquidity risk, reduce excessive debt yields,and increase solvency uncertainty (a measure of portfoliovolatility index). For the method in [6], Petrone and Latora\\nbelieve that the interconnectedness of ﬁnancial institutions\\nwill aﬀect instability and credit crises. In order to quantify\\nsystemic risks, research shows that this mechanism is highly\\ncontagious; that is, the lower the correlation betweenHindawi\\nWireless Communications and Mobile Computing\\nVolume 2021, Article ID 4811086, 7 pages'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 0}, page_content='contagious; that is, the lower the correlation betweenHindawi\\nWireless Communications and Mobile Computing\\nVolume 2021, Article ID 4811086, 7 pages\\nhttps://doi.org/10.1155/2021/4811086'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 1}, page_content='bankrupt banks, the greater the loss. This is in sharp contrast\\nwith the di ﬀerent advantages of standardized credit risk\\nmodels adopted by banks and regulatory agencies. Therefore,this may depreciate the capital needed to overcome the crisisand lead to instability of the ﬁnancial system [6]. And this\\nmechanism has a negative impact on consumer ﬁnance.\\nAolin believes that joint loan guarantee agreements andmutual guarantee agreements between SMEs form the basisof the SME guarantee network. Therefore, a risk control plan\\nis formulated according to the situation and importance of\\nthe company in the network. Use real-time mortgage datato determine the company ’s node location on the mortgage\\nnetwork (including Coriolis and near the company) tounderstand the protection mechanism and prevent systemiccredit risks before the crisis [7].\\nThe innovations of this paper are as follows: (1) Internet\\nﬁnancial risk prevention strategies. From the unique perspec-'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 1}, page_content='The innovations of this paper are as follows: (1) Internet\\nﬁnancial risk prevention strategies. From the unique perspec-\\ntive of big data, the actual business big data is applied to per-sonal credit risk assessment, and method theory is derived tobetter provide services for Internet ﬁnancial risk control. (2)\\nUse the machine learning algorithm model to conveniently\\nconstruct an individual credit risk assessment index system.\\nXGBoost machine learning algorithm method applicationinnovation and XGBoost in the processing of large amountsof data; its distributed, parallel computing; and GPU graphicscard computing advantages signi ﬁcantly improve model\\ntraining e ﬃciency and can output feature importance scores,\\nfor noncorrelation. The index is ﬁltered to facilitate the rapid\\nestablishment of a personal credit risk assessment index sys-tem.(3) Using the big data technology platform, the ﬁnancial'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 1}, page_content='for noncorrelation. The index is ﬁltered to facilitate the rapid\\nestablishment of a personal credit risk assessment index sys-tem.(3) Using the big data technology platform, the ﬁnancial\\nindustry can establish a comprehensive risk view, and man-agement and risk managers can understand the risk view oftheﬁnancial industry from di ﬀerent dimensions and grasp\\nthe control status of its main risk points.\\n2. Proposed Method\\n2.1. Big Data Credit. Credit evaluation in the traditional sense\\nis mainly based on the interbank ﬁnancial lending relation-\\nship; that is, based on the historical economic data andbehavior of the debtor, the overall credit level of the debtoris evaluated by a simple linear analysis. It can be seen thatthe advantage of this method of analyzing historical informa-tion based on the individual user is that the credibility of the\\ndata is often high, so the risk control is also relatively good'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 1}, page_content='data is often high, so the risk control is also relatively good\\n[8]. In addition, due to the small dimensions of data collec-tion, the time lag e ﬀect is obvious, and credit service products\\nare relatively few [9, 10]. The emergence of big data haslargely overturned this traditional credit reference modeland concept. Big data credit can be regarded as “big data\\ntechnology + traditional credit, ”which uses computer and\\nInternet technology to support the model analysis and pro-cessing of data analysis and processing involved in creditinformation activities, which can more fully re ﬂect the eco-\\nnomic activities and the credit situation involved [11–13].\\nFrom the perspective of China ’s current credit informa-\\ntion system, there are great di ﬀerences in the sources, statis-\\ntical calibers, and methods of processing credit information.From the perspective of foreign countries, it was originallyonly a ﬁnancial product that provided online alternatives'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 1}, page_content='tical calibers, and methods of processing credit information.From the perspective of foreign countries, it was originallyonly a ﬁnancial product that provided online alternatives\\nbased on credit information. Later, it was the ﬁrst to intro-\\nduce big data technology in the industry, classify datasources, and use computers to perform simulation analysisto reduce credit risk. Control capacity has increased by about40%, and credit information service e ﬃciency has increased\\nby nearly 90%.\\n2.2. Theory of Information Economics. Information econom-\\nics is based on a society with a certain level of information'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 1}, page_content='by nearly 90%.\\n2.2. Theory of Information Economics. Information econom-\\nics is based on a society with a certain level of information\\ntechnology development. Information economists believethat information is another important resource in the marketin addition to labor and capital. Information is also an ele-ment of transaction costs. People who are in a strong positionof information can obtain excess returns, while those who areweak in information pay excess costs, which will lead to a lackof fairness in market transactions, increase the cost of trans-actions, and ultimately reduce market e ﬃciency [14, 15].\\nThe personal credit information system can well solve the\\nproblem of adverse selection and moral hazard in informa-tion asymmetry, and it is an e ﬀective system that can trans-'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 1}, page_content='The personal credit information system can well solve the\\nproblem of adverse selection and moral hazard in informa-tion asymmetry, and it is an e ﬀective system that can trans-\\nmit signals and punish unbelievers and encouragetrustworthy people. The information sharing platform pro-vided by the personal credit information system can allowthe lender (such as a bank) to have a deep understanding ofthe credit status of the borrower before making a loan deci-sion, e ﬀectively eliminating the problem of adverse selection,\\nand after the loan, the penalty for personal credit informationThe existence of mechanisms can prevent the occurrence ofmoral hazard. From the perspective of the borrower, the bor-rower continuously improves its information in the personalcredit information system and then improves its credit status\\nby keeping promises, so that it can obtain a more favorable'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 1}, page_content='by keeping promises, so that it can obtain a more favorable\\ninterest rate in future loans [16, 17]. In the long run, theestablishment and improvement of the personal credit infor-mation system are bene ﬁcial not only to both borrowers and\\nlenders on the micro level but also to the healthy develop-ment of the entire market economy on the macro level.\\n2.3. Credit and Credit. Credit information in English is\\n“credit reporting ”or“credit investigation,” and its speci ﬁc\\nmeaning is to collect credit data from nature. From this per-\\nspective, credit information is a series of activities with pur-poseful and directional information collection, processing,\\nand evaluation [18, 19]. Literally, credit also means honesty\\nand trust. It can be seen that credit often appears in the moraland ethical dimensions of economic life and is a relativelybroad concept [20]. On a narrower level, credit speci ﬁcally'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 1}, page_content='and trust. It can be seen that credit often appears in the moraland ethical dimensions of economic life and is a relativelybroad concept [20]. On a narrower level, credit speci ﬁcally\\nrefers to the behavior of the goods or currency holder (cred-itor) to provide the borrower (debtor) with the goods or cur-rency under the condition that the other party promises torepay, which is essentially a debt-debt relationship. There-fore, credit can be understood as the debtor ’s ability to repay,\\nas well as credit and trust activities based on commodities orcurrencies [21, 22].\\n2.4. Construction and Optimization of Personal Credit Risk\\nAssessment Index System Based on Big Data. With the contin-\\nuous improvement of the personal credit system, personal2 Wireless Communications and Mobile Computing'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 1}, page_content='Assessment Index System Based on Big Data. With the contin-\\nuous improvement of the personal credit system, personal2 Wireless Communications and Mobile Computing\\n 6302, 2021, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2021/4811086 by Readcube (Labtiva Inc.), Wiley Online Library on [21/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 2}, page_content='credit behavior has gradually been recognized and become a\\nnecessary means to re ﬂect personal morality and maintain\\nsocial and economic order. It can be seen that personal creditrisk assessment is of great signi ﬁcance for both commercial\\nbanks and residents. The construction of indicators requireshigh matching rate, high saturation, good timeliness, andmultidimensional data. In this way, indicators are compara-\\ntive and easy to understand, and it is easy to understand\\nthe actual situation of personal credit. The data studied inthis project comes from the company ’s big data platform.\\nThe data of the big data platform comes from multiplesources, such as company APP product data, central bankcredit data, Internet credit company credit data, and e-commerce company ’s online shopping data. Data are from'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 2}, page_content='three-party cooperative enterprises, Internet public data cap-tured by crawlers, and data released by public inspection law.The data used in the research of this project are desensitizedfrom multiple levels to ensure data security [23, 24]. The con-struction of a personal credit risk assessment indicator sys-\\ntem requires engineering steps such as data exploration and\\npreprocessing, feature engineering, preliminary screening ofindicators, and optimization of indicators. The speci ﬁc data\\nﬂow diagram is shown in Figure 1.\\n2.5. Construction and Optimization of Personal Credit RiskAssessment Model Based on Big Data Index System. The main\\nidea of logistic regression for classiﬁ cation is to establish a\\nclassiﬁcation formula based on existing data to establish a\\nregression formula for classiﬁ cation boundaries. The purpose\\nof logistic regression is to ﬁnd the best- ﬁtting parameters of'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 2}, page_content='classiﬁcation formula based on existing data to establish a\\nregression formula for classiﬁ cation boundaries. The purpose\\nof logistic regression is to ﬁnd the best- ﬁtting parameters of\\nthe nonlinear function sigmoid, which can be done by opti-mization algorithms [25, 26]. Among the optimization algo-rithms, the most commonly used is the gradient ascentalgorithm. The gradient ascent algorithm can be simpliﬁ ed\\nto a stochastic gradient ascent algorithm. The stochastic gra-dient ascent algorithm is equivalent to the gradient ascentalgorithm, but it occupies less computing resources [27]. Inaddition, stochastic gradient ascent is an online algorithmthat updates parameters when new data arrives without hav-ing to reread the entire data set of the batch operation. Theadvantage of logistic regression is that linear calculation is\\nlow in cost, easy to understand, and simple to implement.\\nThe disadvantage is that it is easy to under ﬁt, and the classi-'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 2}, page_content='low in cost, easy to understand, and simple to implement.\\nThe disadvantage is that it is easy to under ﬁt, and the classi-\\nﬁcation accuracy may not be high. The applicable data types\\nare numerical and nominal data.The logistic regression model is a binary classi ﬁcation\\nmodel, which can be expressed by a conditional probability dis-tribution pðy=1∣xÞ. The form is a parameterized logistic dis-\\ntribution. Assuming that the vector x=ðx\\n1,x2,x3,⋯,xnÞhasn\\nindependent variables, the condition rate pðy=1∣xÞ=pis the\\nprobability that the observed value occurs for x. Therefore, the\\nlogistic regression model can be expressed as follows:\\npy=1xj ðÞ =πxðÞ=1\\n1+e−gxðÞ: ð1Þ\\nfðxÞ=1 / ð1+ e−gðxÞÞis called logistic function.\\ngxðÞ=w0+w1xx+⋯+wnxn: ð2Þ\\nThe probability that ydoes not occur under xconditions is\\npy=0xj ðÞ =1−py=1xj ðÞ =1−1\\n1+e−gxðÞ=1\\n1+egxðÞ:ð3Þ\\nThe ratio of the probability of occurrence of yto the occur-\\nrence of yis\\npy=1xj ðÞ\\npy=0xj ðÞ=p\\n1−p=egxðÞ: ð4Þ'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 2}, page_content='py=0xj ðÞ =1−py=1xj ðÞ =1−1\\n1+e−gxðÞ=1\\n1+egxðÞ:ð3Þ\\nThe ratio of the probability of occurrence of yto the occur-\\nrence of yis\\npy=1xj ðÞ\\npy=0xj ðÞ=p\\n1−p=egxðÞ: ð4Þ\\nThis ratio is called the occurrence ratio of events (odds), and\\ntaking the log of odds gives\\nInp\\n1−p\\x12\\x13\\n=gxðÞ=w0+w1x1+⋯+wnxn: ð5Þ\\nIt can be seen that the dependent variable and the indepen-\\ndent variable are nonlinear relati onships, and the linear conver-\\nsion can be performed by odds ratio of logarithmic occurrence.\\nUsually, the maximum likelihood estimation is used to\\nﬁnd the parameters of the logistic model. The likelihood\\nfunction isData exploration and\\npreprocessingFeature engineeringScreening of \\npreliminary indicatorsIndicator optimization\\nEDA data exploration\\nData cleaning\\nData standardization\\nSample groupingBasic feature extraction\\nComplex feature\\nconstructionDimensional disaster\\nNullrate filtering\\nDeduplicationFiltered optimization\\nInclusive optimization\\nGenerate final metrics'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 2}, page_content='Sample groupingBasic feature extraction\\nComplex feature\\nconstructionDimensional disaster\\nNullrate filtering\\nDeduplicationFiltered optimization\\nInclusive optimization\\nGenerate final metrics\\nFigure 1: Steps to build a personal credit risk assessment indicator system.3 Wireless Communications and Mobile Computing\\n 6302, 2021, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2021/4811086 by Readcube (Labtiva Inc.), Wiley Online Library on [21/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 3}, page_content='LwðÞ=Yn\\n1πxiðÞðÞyi1−πxiðÞ ðÞ1−yi: ð6Þ\\nThe basic idea of applying the logistic model to personal\\ncredit risk assessment is as follows: a sample of ðXi1,Xi2,\\nXi3,⋯,Xin:YiÞði=1 ,2 ,3 , ⋯,kÞsample data of ngroups of\\nloan customers is given, where Yis a 0-1 variable and Yi=\\n1indicates that the ith customer is a bad credit customer.\\nThe logistic equation is\\nPi=expβ0+β1Xi1+β2Xi2+β3Xi3+⋯+βnXin ðÞ\\n1+β0+β1Xi1+β2Xi2+β3Xi3+⋯+βnXin ðÞ: ð7Þ\\nThe above equation can be linearly changed to obtain\\nInPi\\n1−Pi\\x12\\x13\\n=β0+β1Xi1+β2Xi2+β3Xi3+⋯+βnXin,\\u2003i=1 ,2 , ⋯,n:\\nð8Þ\\nY1,Y2,Y3⋯Ynis an independent binary classi ﬁcation\\nvariable. Let fiðYiÞrepresent the probability of Yi=1orYi\\n=0; then, its likelihood function (LF) is\\nInf Y1,⋯,Yn ðÞi=〠n\\n1YiInPi+1−Yi ðÞ In 1−P ðÞ ðÞ ½\\x8a : ð9Þ\\nFor ease of use, take the natural logarithms on both sides\\nof formula (9), and obtain the log-likelihood function (LLF) as\\nInf Y1,⋯,Yn ðÞ =〠n\\n1YiInPi+1−Yi ðÞ 1−Pi ðÞ ½\\x8a\\n=〠n\\n1YiInPi−YiIn 1−Pi ðÞ +I n 1 −Pi ðÞ ½\\x8a\\n=〠n\\n1YiInPi\\n1−Pi\\x12\\x13\\n+〠n\\n1In 1−Pi ðÞ\"#'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 3}, page_content='of formula (9), and obtain the log-likelihood function (LLF) as\\nInf Y1,⋯,Yn ðÞ =〠n\\n1YiInPi+1−Yi ðÞ 1−Pi ðÞ ½\\x8a\\n=〠n\\n1YiInPi−YiIn 1−Pi ðÞ +I n 1 −Pi ðÞ ½\\x8a\\n=〠n\\n1YiInPi\\n1−Pi\\x12\\x13\\n+〠n\\n1In 1−Pi ðÞ\"#\\n:\\nð10 Þ\\nPutting formula (8) into formula (10), we get\\nInf Y1,⋯,Yn ðÞ =〠n\\niYiβ0+β1Xi1+β2Xi2+⋯+βnXin ðÞ\\n−〠n\\niIn 1 + eβ0+β1Xi1+β2Xi2+⋯+βnXin ðÞhi\\n,\\n\\u2003i=1 ,2 ,3 , ⋯,n:\\nð11 Þ\\nFind the partial derivatives of βi, and make their expres-\\nsions 0, so you get the maximum likelihood estimator.3. Experiments\\n3.1. Experimental Design\\n3.1.1. Index Selection. According to the foregoing theoretical\\nanalysis, we found from the basic information of individual\\nusers in a bank that the borrower ’s basic personal informa-\\ntion and loan-related information will a ﬀect the borrower ’s\\ncredit limit. In view of the fact that all the borrowers ’default'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 3}, page_content='users in a bank that the borrower ’s basic personal informa-\\ntion and loan-related information will a ﬀect the borrower ’s\\ncredit limit. In view of the fact that all the borrowers ’default\\nrecords disclosed by the Renrendai platform are 0 times andthe credit ratings are all grade A, select the total amount ofthe subject as the explanatory variable. The speci ﬁc situation\\nof each indicator is as follows:\\n(1) The total amount of the subject. The total amount of\\nthe target refers to the amount of the loan that theborrower wishes to post on the Renrendai platform.The minimum value is 5250 yuan, and the maximumvalue is 193,500 yuan, so the value range of the target\\ntotal is 5250-193,500. This article uses the target total\\nAct as a proxy for credit lines\\n(2) Age. Age refers to the actual age of the borrower. The\\nyoungest borrower is 22 years old, and the oldest bor-\\nrower is 58 years old. The age range is 22-58\\n(3) Educational background. The borrower ’s education'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 3}, page_content='youngest borrower is 22 years old, and the oldest bor-\\nrower is 58 years old. The age range is 22-58\\n(3) Educational background. The borrower ’s education\\nincludes four levels: graduate or above, undergradu-\\nate, college, and high school or below. This variable\\nis a nominal variable, so it needs to be quanti ﬁed\\nand converted to a dummy variable. Set “high school\\nor below ”as 1, “college ”as 2, and “undergraduate ”as\\n3, and the value of “graduate or above ”is 4\\n(4) Marital status. Borrowers have three types of marital\\nstatus: divorced, unmarried, and married. This variable\\nis nominal, so it needs to be quanti ﬁed and converted to\\na dummy variable. The value of “divorce” is 1, the value\\nof“unmarried ”is 2, and the value of “Married ”is 3\\n3.1.2. Research Hypotheses. Based on the relevant research\\nresults of the existing literature, this paper proposes the fol-lowing research hypotheses:\\nHypothesis 1. In terms of the personal characteristics of the'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 3}, page_content='results of the existing literature, this paper proposes the fol-lowing research hypotheses:\\nHypothesis 1. In terms of the personal characteristics of the\\nborrower, the older the borrower, the higher the credit limit;\\nthe higher the borrower ’s education, the higher the credit\\nlimit; the more stable the marriage status of the borrower,the higher the credit limit; the larger the credit limit, thehigher the borrower ’s job position, the higher the credit limit;\\nthe longer the borrower ’s working time, the higher the credit\\nlimit; the more developed the province of the borrower ’s\\neconomy, the higher the credit limit.\\nHypothesis 2. In terms of the ﬁnancial characteristics of the\\nborrower, the higher the borrower ’s income, the higher the\\ncredit limit; if the borrower owns real estate or car produc-\\ntion, the higher the credit limit; if the borrower has the mort-gage or car loan, the lower the credit limit.4 Wireless Communications and Mobile Computing'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 3}, page_content='tion, the higher the credit limit; if the borrower has the mort-gage or car loan, the lower the credit limit.4 Wireless Communications and Mobile Computing\\n 6302, 2021, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2021/4811086 by Readcube (Labtiva Inc.), Wiley Online Library on [21/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 4}, page_content='Hypothesis 3. In terms of the creditworthiness of the bor-\\nrower, the more times the borrower successfully applies for\\na loan, the higher the credit limit obtained; the more the bor-rower pays o ﬀ, the higher the credit limit.\\nHypothesis 4. In terms of borrowing characteristics of bor-\\nrowers, the higher the annual interest rate, the lower the creditline; the longer the repayment period, the lower the credit line.\\n3.1.3. Credit Report. The information in the personal credit\\nreport mainly includes six aspects: the results of the identity\\ninformation veri ﬁcation by the Ministry of Public Security,\\nbasic personal information, bank credit transaction informa-tion, nonbank credit information, personal declarations andobjections, and query historical information.\\n4. Discussion\\n4.1. Regression Results of Factors A ﬀecting Credit Lines\\n4.1.1. Personal Characteristics. As shown in Table 1 and'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 4}, page_content='4. Discussion\\n4.1. Regression Results of Factors A ﬀecting Credit Lines\\n4.1.1. Personal Characteristics. As shown in Table 1 and\\nFigure 2, among the several variables representing personalcharacteristics, the three variables of marital status, workinghours, and working place have no signi ﬁcant e ﬀect on the\\nborrower ’s credit line. Age is statistically signi ﬁcant at a sig-\\nniﬁcant level of 1%, and the sign of the coeﬃ cient is positive,\\nindicating that the age of the borrower has a positive impacton its credit limit. The older the borrower is, the larger theamount of borrowing that can be obtained. Older borrowerstend to have richer social experience and richer asset accu-mulation, so the larger the amount of borrowing they can get.\\n4.2. Financial Characteristics. As shown in Figure 3, the\\nexperimental object of the survey is my country ’s multiple\\nvariables representing ﬁnancial characteristics in 2014-\\n2019; whether or not owning a real estate or car production'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 4}, page_content='experimental object of the survey is my country ’s multiple\\nvariables representing ﬁnancial characteristics in 2014-\\n2019; whether or not owning a real estate or car production\\nhas no signi ﬁcant e ﬀect on its credit line. When the borrower\\ncannot repay on time, it is di ﬃcult to conduct mortgage auc-\\ntions on the borrower ’s real estate and car properties, so\\nwhether or not owning the real estate or car properties hasno obvious e ﬀect on the borrower ’s credit limit.\\nHousing loans and car loans are statistically signi ﬁcant at\\nsigniﬁcant levels of 5% and 10%, respectively, and the signs of\\nthe coeﬃ cients are positive, indicating that housing loans and\\ncar loans have a positive impact on borrowers ’credit lines.'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 4}, page_content='signiﬁcant levels of 5% and 10%, respectively, and the signs of\\nthe coeﬃ cients are positive, indicating that housing loans and\\ncar loans have a positive impact on borrowers ’credit lines.\\nOn the contrary, borrowers who have a home loan or car loancan also obtain a higher amount of borrowing because inves-tors have very limited information about the borrower. If theborrower can obtain a house loan or car loan through bankinspection, it means good solvency, so the amount of borrow-ing that can be obtained may also be higher.\\n4.3. Empirical Analysis of Personal Credit Risk Assessment\\nBased on Big Data Platform\\n4.3.1. Analysis of the Company ’s Personal Credit Risk\\nAssessment Results. Using the above-built logistic regression180 25\\n20\\n15\\n10\\n5\\n0160\\n140\\n120Quantity\\n100\\n80\\nAbsolute amount2014 2015 2016 2017 2018 2019\\nGrowth rate\\nFigure 3: Statistics of ﬁnancial characteristics.⨯104\\n4\\n3\\n2\\n12 3 45 6 71\\n0\\n–1\\n–2\\n–3\\nCoefficient\\nStandard\\n95% confidence interval'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 4}, page_content='100\\n80\\nAbsolute amount2014 2015 2016 2017 2018 2019\\nGrowth rate\\nFigure 3: Statistics of ﬁnancial characteristics.⨯104\\n4\\n3\\n2\\n12 3 45 6 71\\n0\\n–1\\n–2\\n–3\\nCoefficient\\nStandard\\n95% confidence interval\\nFigure 2: Regression results of credit line in ﬂuencing factors.Table 1: Regression results of credit line in ﬂuencing factors.\\nVolume Coeﬃcient Standard tvalue p>∣t∣ 95% con ﬁdence interval\\nAge 1098.593 219.4229 5.01 0.0000∗∗∗667.8678 1529.318\\nEducation 9673.968 2870.089 3.37 0.001∗∗∗4040.01 15,307.93\\nMarital status -711.139 2468.392 -0.29 0.773 -5556.569 4134.292\\nCompany size -25,407 3121.991 8.14 0.000∗∗∗31,535.39 19,278.52\\nWork position 4330.822 669.6249 6.47 0.000∗∗∗3016.355 5645.29\\nOperating hours -470.134 1572.361 -0.3 0.765 -3556.663 2616.395\\nWork place -324.846 1833.539 -0.18 0.859 -3924.066 3274.3735 Wireless Communications and Mobile Computing'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 4}, page_content='Operating hours -470.134 1572.361 -0.3 0.765 -3556.663 2616.395\\nWork place -324.846 1833.539 -0.18 0.859 -3924.066 3274.3735 Wireless Communications and Mobile Computing\\n 6302, 2021, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2021/4811086 by Readcube (Labtiva Inc.), Wiley Online Library on [21/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 5}, page_content='+ XGBoost decision tree fusion model to make empirical\\npredictions on the company ’s personal credit data, there were\\n29,171 loan customers, 22,470 normal customers, and 6,701\\ndefault customers.\\nAs shown in Table 2, compared with the single model, the\\npersonal credit risk assessment model based on the fusionmodel in the company ’s big data business environment has\\nhigher classi ﬁcation accuracy, can well predict normal cus-\\ntomers and default customers, and is suitable for practicalindividual ’s credit risk control business. These characteristics\\nare highly interpretable and can be cross-validated with theindustry ’s professional knowledge, which further promotes\\nthe improvement of personal credit business and facilitatesthe construction of a comprehensive personal credit riskassessment system in the Internet ﬁnance industry.\\nAs shown in Figure 4, the AUC of this project fusion'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 5}, page_content='As shown in Figure 4, the AUC of this project fusion\\nmodel is 0.93 and KS is 0.66; it shows that the predictive abil-ity of the personal credit risk assessment model is good, andit can accurately distinguish between normal loan customersand default loan customers, which is suitable for practicalpersonal credit risk control in business. The prediction accu-racy of the default model of the fusion model is 97.14%, andthe default rate corresponding to the actual business is 2.86%.The default rate of personal credit loans was well controlled,\\nfrom the original default rate of 4% to 2.86%, and the default\\nrate decreased by 1.14%.\\n5. Conclusions\\nCurrently, most Chinese companies are facing many chal-lenges in terms of data infrastructure system architectureand data analysis. In the context of big data and the Internet,\\nbig data credit reporting and consumer ﬁnance have devel-\\noped rapidly. The prerequisite for the healthy development\\nof the consumer ﬁnance industry is e ﬃcient and accurate'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 5}, page_content='big data credit reporting and consumer ﬁnance have devel-\\noped rapidly. The prerequisite for the healthy development\\nof the consumer ﬁnance industry is e ﬃcient and accurate\\ncredit risk management. In the context of the timelinessand comprehensiveness, this paper studies and analyzes thelimitations of the classic credit risk rating system, avoidsthe limitations of traditional indicators when designing apersonal big data risk rating system, and creates completeand logical personal credit rating data. In the context of sys-tem, after the index is created for the ﬁrst time, the index may\\nhave relevance issues and unequal predictability. Therefore,the focus of research is to use e ﬀective feature selection\\nmethods to optimize indicators when creating indicators.\\nData Availability\\nNo data is available. The article does not touch on data.\\nConflicts of Interest\\nThe authors declare that they have no con ﬂicts of interest.\\nReferences'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 5}, page_content='Data Availability\\nNo data is available. The article does not touch on data.\\nConflicts of Interest\\nThe authors declare that they have no con ﬂicts of interest.\\nReferences\\n[1] D. V. Kousenidis, A. C. Ladas, and C. I. Negkakis, “Aggregate\\naccounting data and the prediction of credit risk,” Interna-\\ntional Journal of Accounting , vol. 54, no. 1, article 1950001,\\n2019.\\n[2] T. Schmitt, A. Schmitt, R. Schäfer, and T. Guhr, “Credit risk:\\ntaking ﬂuctuating asset correlations into account, ”Journal of\\nCredit Risk , vol. 11, no. 3, pp. 73 –94, 2016.\\n[3] Y. Sun, R. Mendoza-Arriaga, and V. Linetsky, “Marshall –\\nOlkin distributions, subordinators, eﬃ cient simulation, and\\napplications to credit risk, ”Advances in Applied Probability ,\\nvol. 49, no. 2, pp. 481 –514, 2017.\\n[4] S. Wang, Y. Qi, and F. Bin, “Credit risk evaluation based on\\ntext analysis, ”International Journal of Cognitive Informatics\\nand Natural Intelligence , vol. 10, no. 1, pp. 1–11, 2016.'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 5}, page_content='[4] S. Wang, Y. Qi, and F. Bin, “Credit risk evaluation based on\\ntext analysis, ”International Journal of Cognitive Informatics\\nand Natural Intelligence , vol. 10, no. 1, pp. 1–11, 2016.\\n[5] S. Morris and H. S. Shin, “Illiquidity component of credit risk, ”\\nWorking Papers , vol. 57, no. 4, pp. 1135 –1148, 2016.\\n[6] D. Petrone and V. Latora, “A dynamic approach merging net-\\nwork theory and credit risk techniques to assess systemic risk\\ninﬁnancial networks, ”Scienti ﬁc Reports , vol. 8, no. 1,\\np. 5561, 2018.\\n[7] A. Leng, G. Xing, and W. Fan, “Credit risk transfer in SME\\nloan guarantee networks, ”Journal of Systems Science & Com-\\nplexity , vol. 30, no. 5, pp. 1084 –1096, 2017.\\n[8] Y.-H. Yuan, S.-H. Tsao, J.-T. Chyou, and S.-B. Tsai, “An\\nempirical study on eﬀ ects of electronic word-of-mouth and\\ninternet risk avoidance on purchase intention: from theTable 2: Forecast results of personal credit risk model of personal credit business data.\\nModel nameActual normal\\ncustomersActual default'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 5}, page_content='internet risk avoidance on purchase intention: from theTable 2: Forecast results of personal credit risk model of personal credit business data.\\nModel nameActual normal\\ncustomersActual default\\ncustomersPredict the number of\\nnormal customersPredicted default\\ncustomersAccuracy in predicting\\ndefault customers\\nLogistic + XGBoost\\nfusion model22,470 6701 19,738 6509 97.14%\\n1\\n0.9\\n0.8\\n0.7\\n0.6\\n0.5\\n0.4\\n0.30.2\\n0.1\\n0\\n0 0.2 0.4 0.6 0.8\\nROC\\nROC curve1\\nFigure 4: ROC curve of the fusion model.6 Wireless Communications and Mobile Computing\\n 6302, 2021, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2021/4811086 by Readcube (Labtiva Inc.), Wiley Online Library on [21/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 6}, page_content='perspective of big data, ”Soft Computing, vol. 2020, no. 24,\\npp. 5713 –5728, 2020.\\n[9] Z. Zhang, J. He, and G. Gao, “Sparse multi-criteria optimiza-\\ntion classiﬁ er for credit risk evaluation, ”Soft Computing,\\nvol. 23, no. 1, pp. 1–14, 2017.\\n[10] M. Y. Kang and M. Ausloos, “An inverse problem study: credit\\nrisk ratings as a determinant of corporate governance and cap-\\nital structure in emerging markets: evidence from Chinese\\nlisted companies, ”Economies , vol. 5, no. 4, p. 47, 2017.\\n[11] G. F. Amanollahi, “The in ﬂuence of external factors on the\\ncredit risk in leasing industry,” Management Science Letters ,\\nvol. 6, no. 3, pp. 251 –258, 2016.\\n[12] W. Wu and D. G. McMillan, “Dynamic linkages in credit risk:\\nmodeling the time-varying correlation between the money and\\nderivatives markets over the crisis period, ”Journal of Risk ,\\nvol. 16, no. 2, pp. 51 –59, 2013.\\n[13] J. Skoglund and W. Chen, “The application of credit risk'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 6}, page_content='derivatives markets over the crisis period, ”Journal of Risk ,\\nvol. 16, no. 2, pp. 51 –59, 2013.\\n[13] J. Skoglund and W. Chen, “The application of credit risk\\nmodels to macroeconomic scenario analysis and stress test-ing, ”Journal of Credit Risk , vol. 12, no. 2, pp. 1–45, 2016.\\n[14] M. Punniyamoorthy and P. Sridevi, “Identi ﬁcation of a stan-\\ndard AI based technique for credit risk analysis, ”Benchmark-\\ning An International Journal , vol. 23, no. 5, pp. 1381 –1390,\\n2016.\\n[15] J. Skoglund and W. Chen, “Rating momentum in the macro-\\neconomic stress testing and scenario analysis of credit risk,”\\nThe Journal of Risk Model Validation , vol. 11, no. 1, pp. 21 –\\n47, 2016.\\n[16] J. Eckert, K. Jakob, and M. Fischer, “A credit portfolio frame-\\nwork under dependent risk parameters: probability of default,loss given default and exposure at default,” Journal of Credit\\nRisk, vol. 12, no. 1, pp. 97 –119, 2016.\\n[17] Z. Qing, J.-j. Yang, and W.-x. Wu, “Pricing vulnerable options'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 6}, page_content='Risk, vol. 12, no. 1, pp. 97 –119, 2016.\\n[17] Z. Qing, J.-j. Yang, and W.-x. Wu, “Pricing vulnerable options\\nwith correlated credit risk under jump-di ﬀusion processes\\nwhen corporate liabilities are random, ”Acta Mathematicae\\nApplicatae Sinica , vol. 35, no. 2, pp. 305 –318, 2019.\\n[18]\\nH. Altinbas and G. C. Akkaya, “Improving the performance of\\nstatistical learning methods with a combined meta-heuristicfor consumer credit risk assessment,” Risk Management ,\\nvol. 19, no. 4, pp. 255 –280, 2017.\\n[19] S. Yamanaka, H. Nakagawa, and M. Sugihara, “Random thin-\\nning with credit quality vulnerability factor for better risk\\nmanagement of credit portfolio in a top-down framework, ”\\nJapan Journal of Industrial and Applied Mathematics, vol. 33,no. 2, pp. 321 –341, 2016.\\n[20] Y. Lee and S.-H. Poon, “Modeling the credit contagion channel\\nand its consequences via the standard portfolio credit risk\\nmodel, ”Journal of Credit Risk , vol. 10, no. 1, pp. 33–62, 2016.'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 6}, page_content='[20] Y. Lee and S.-H. Poon, “Modeling the credit contagion channel\\nand its consequences via the standard portfolio credit risk\\nmodel, ”Journal of Credit Risk , vol. 10, no. 1, pp. 33–62, 2016.\\n[21] F. Kaposty, M. Löderbusch, and J. Maciag, “Stochastic loss\\ngiven default and exposure at default in a structural model of\\nportfolio credit risk, ”Social Science Electronic Publishing ,\\nvol. 13, no. 1, pp. 93 –123, 2017.\\n[22] I. Basoglu, W. Hormann, and H. Sak, “Eﬃcient simulations for\\na Bernoulli mixture model of portfolio credit risk,” Annals of\\nOperations Research , vol. 260, no. 1-2, pp. 113 –128, 2018.\\n[23] L. Bo and A. Capponi, “Optimal investment in credit deriva-\\ntives portfolio under contagion risk,” Mathematical Finance,\\nvol. 26, no. 4, pp. 785 –834, 2016.\\n[24] J. Lee, D. H. Lee, and S.-G. Yun, “Systemic risk on trade credit\\nsystems: with the tangible interconnectedness, ”Computa-'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 6}, page_content='vol. 26, no. 4, pp. 785 –834, 2016.\\n[24] J. Lee, D. H. Lee, and S.-G. Yun, “Systemic risk on trade credit\\nsystems: with the tangible interconnectedness, ”Computa-\\ntional Economics , vol. 51, no. 2, pp. 211 –226, 2018.[25] D. Froneberg, F. Kiesel, and D. Schiereck, “CDS and bank\\nownership structures: does the credit side show who advocatesmore risk?, ”Journal of Risk Finance , vol. 17, no. 2, pp. 169 –\\n193, 2016.\\n[26] F. Lin, S.-y. Xie, and J.-p. Yang, “Semi-analytical formula for\\npricing bilateral counterparty risk of CDS with correlatedcredit risks, ”Acta Mathematicae Applicatae Sinica , vol. 34,\\nno. 2, pp. 209 –236, 2018.\\n[27] M. Reddy, “Smart vehicular system based on the Internet of\\nThings,” Journal of Organizational and End User Computing ,\\nvol.30,\\nno. 3, pp. 45 –62, 2018.7 Wireless Communications and Mobile Computing'),\n",
       " Document(metadata={'source': 'data/4.pdf', 'page': 6}, page_content='Things,” Journal of Organizational and End User Computing ,\\nvol.30,\\nno. 3, pp. 45 –62, 2018.7 Wireless Communications and Mobile Computing\\n 6302, 2021, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2021/4811086 by Readcube (Labtiva Inc.), Wiley Online Library on [21/10/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "final_docs = pdf_splitter.split_documents(pdf_documents)\n",
    "final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db  = FAISS.from_documents(final_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local('faiss_index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
